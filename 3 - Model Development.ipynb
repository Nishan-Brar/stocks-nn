{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:47.124593Z",
     "start_time": "2023-01-05T12:27:38.357356Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from preprocess_data import preprocess_data\n",
    "from functools import partial\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation, Normalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:47.153857Z",
     "start_time": "2023-01-05T12:27:47.124593Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Check GPU is detected\n",
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import stock price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:52.288577Z",
     "start_time": "2023-01-05T12:27:47.211965Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "stock_prices_df = pd.read_csv(\"data/price.csv\")\n",
    "\n",
    "# Set date-time index\n",
    "stock_prices_df['Date'] = pd.to_datetime(stock_prices_df[\"Date\"], format='%Y-%m-%d')\n",
    "stock_prices_df = stock_prices_df.set_index(stock_prices_df['Date'])\n",
    "stock_prices_df = stock_prices_df.drop(columns = [\"Date\"])\n",
    "\n",
    "# Drop all columns which only contain nan values\n",
    "boolean = []\n",
    "for stock in stock_prices_df.columns:\n",
    "    boolean.append(not stock_prices_df[stock].isnull().all())\n",
    "stock_prices_df = stock_prices_df.iloc[:, boolean]\n",
    "\n",
    "stock_prices_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "cat_features = pd.read_csv(\"data/categorical.csv\")\n",
    "cat_features = cat_features.set_index(\"Unnamed: 0\")\n",
    "cat_features = cat_features.rename_axis(None, axis = 0)\n",
    "cat_features = cat_features.dropna(axis=\"columns\")\n",
    "\n",
    "# Drop stocks such that the remaining stocks are those in both `cat_features` and `stock_prices_df`\n",
    "intersection = list(set(stock_prices_df.columns) & set(cat_features.columns))\n",
    "cat_features = cat_features[intersection]\n",
    "stock_prices_df = stock_prices_df[intersection]\n",
    "\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.897130Z",
     "start_time": "2023-01-05T12:27:54.883312Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get unique categories for each categorical variable\n",
    "unique_cats_df = pd.DataFrame()\n",
    "for cat in cat_features.index:\n",
    "    uniques = list(cat_features.loc[cat, :].unique())\n",
    "    categories = np.sort(uniques)\n",
    "    unique_cats_df[cat] = categories\n",
    "\n",
    "unique_cats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.040653Z",
     "start_time": "2023-01-05T12:27:54.970478Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "num_features = pd.read_csv(\"data/features.csv\")\n",
    "num_features['Date'] = pd.to_datetime(num_features[\"Date\"], format='%Y-%m-%d')\n",
    "num_features = num_features.set_index(\"Date\")\n",
    "num_features = num_features.dropna()\n",
    "\n",
    "# Make names smaller and more manageable\n",
    "print(num_features.columns)\n",
    "\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays for speedy opterations later on\n",
    "stock_prices_np = stock_prices_df.to_numpy()\n",
    "cat_features_np = cat_features.to_numpy()\n",
    "features_daily_np = num_features.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Folders to Store Plots and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create folders to store the models and corresponding plots showing the training process of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.160287Z",
     "start_time": "2023-01-05T12:27:55.130566Z"
    }
   },
   "outputs": [],
   "source": [
    "root_folder = \"results\"\n",
    "overview_folder = \"overview\"\n",
    "details_folder = \"details\"\n",
    "\n",
    "# Create a folder to store models and corresponding plots regarding training and validation details\n",
    "graphs_path = f'{root_folder}/{details_folder}/training & validation details'\n",
    "try:\n",
    "    os.makedirs(graphs_path)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "models_path = f'{root_folder}/{details_folder}/models'\n",
    "try:\n",
    "    os.makedirs(models_path)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Create a csv file to store the hyperparameters used for each model as well as the corresponding loss and r-squared scores\n",
    "try:\n",
    "    os.makedirs(f\"{root_folder}/{overview_folder}\") # Create the folder where this csv file will be stored.\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "\n",
    "model_results_path = f\"{root_folder}/{overview_folder}/model results.csv\"\n",
    "try:\n",
    "    results = pd.read_csv(model_results_path)\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist then we create the file\n",
    "    with open(model_results_path, \"w\") as f:\n",
    "        f.write(\",\".join([\"features\",\"lines\", \"years\", \"layers\", \"neurons per layer\", \"total neurons\", \"batch size\", \"technique\", \"model name\",\"val_loss\", \"val_r2\"]) + \"\\n\")\n",
    "    results = pd.read_csv(model_results_path)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Function to Plot Training & Results of Each Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot allows us to visualise the training process as well as visualise the predictions against the true values using a scatterplot. These plots are created for each model and can be found in `results\\details\\training & validation details`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.253812Z",
     "start_time": "2023-01-05T12:27:55.240604Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot(history, y_test, result_index, test_predictions, R2_score):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13, 5), dpi=300)\n",
    "\n",
    "    # PLot actual vs prediction scatter plot\n",
    "    test_predictions = test_predictions.flatten()\n",
    "    ax[0].scatter(y_test, test_predictions, alpha=0.01)\n",
    "    ax[0].set_xlabel('True Values')\n",
    "    ax[0].set_ylabel('Predictions')\n",
    "\n",
    "    # Plot y=x line\n",
    "    y_equal_x_line = [0, max(max(y_test), max(test_predictions))]\n",
    "    ax[0].plot(y_equal_x_line, y_equal_x_line, color=\"green\", linewidth = 3)\n",
    "    ax[0].set_title(\"Predictions Against True Values\")\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].set_xscale('log')\n",
    "\n",
    "    # Plot training and testing r-squared against epochs\n",
    "    ax[1].plot(history.history['loss'], label='loss')\n",
    "    ax[1].plot(history.history['val_loss'], label='val_loss')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Error')\n",
    "    ax[1].set_title('loss')\n",
    "\n",
    "    fig.suptitle(f\"Val R2 = {R2_score}, Val loss={round(history.history['val_loss'][-1],3)}, result index={result_index}\", fontsize=\"small\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Function to Pre-process Data for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function allows us to pre-process and create the data for the models using several different hyperparameters such as `number_of_lines`, `number_of_years_to_consider`, `cats_subset` and `features_subset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_data(cat_features_subset, num_features_subset, number_of_years_to_consider,number_of_lines):\n",
    "    # Sort for consistency\n",
    "    cat_features_subset.sort()\n",
    "    num_features_subset.sort()\n",
    "\n",
    "    # Function to get the column-index of a given feature within the `num_features` df\n",
    "    def get_num_features_index(feature):\n",
    "        return list(num_features.columns).index(feature)\n",
    "    # Function to get the column-index of a given feature within the `cat_features` df\n",
    "    def get_cat_features_index(cat):\n",
    "        return list(cat_features.index).index(cat)\n",
    "\n",
    "    # Get corresponding column-indexes in `num_features` and `cat_features` for the numerical and categorical features which will be used to train the model \n",
    "    cats_subset_indexes = []\n",
    "    for var in cat_features_subset:\n",
    "        cats_subset_indexes.append(get_cat_features_index(var))\n",
    "    features_subset_indexes = []\n",
    "    for var in num_features_subset:\n",
    "        features_subset_indexes.append(get_num_features_index(var))\n",
    "\n",
    "\n",
    "    # Use multiprocessing with the hyperparameters `number_of_lines`, `number_of_years_to_consider`, `cats_subset` and `features_subset` \n",
    "    # in order to create the data for the model.\n",
    "    range_iterations = range(number_of_years_to_consider*365+1, # `+1` needed since slicing is end-exclusive\n",
    "                              len(stock_prices_np) - 365)\n",
    "    iterations = tqdm(range_iterations,\n",
    "                        desc=f\"Working on Dataset: {number_of_lines} lines, {number_of_years_to_consider} years\",\n",
    "                        disable=False)\n",
    "    p = Pool(os.cpu_count())\n",
    "    result_list = p.map(partial(preprocess_data,\n",
    "                                stocks_np = stock_prices_np,\n",
    "                                number_of_years_to_consider = number_of_years_to_consider,\n",
    "                                number_of_lines = number_of_lines,\n",
    "                                features_subset_indexes = features_subset_indexes,\n",
    "                                features_np = features_daily_np,\n",
    "                                cats_np = cat_features_np,\n",
    "                                cats_index = cats_subset_indexes,\n",
    "                                cats_df = unique_cats_df,\n",
    "                                dates = np.array(stock_prices_df.index),\n",
    "                                stocks = np.array(stock_prices_df.columns)\n",
    "                                ),\n",
    "                        iterations)\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    # Stack corresponding numpy arrays\n",
    "    data = np.vstack([result[0] for result in result_list])\n",
    "    data_dates = np.hstack([result[1] for result in result_list])\n",
    "    data_stocks = np.hstack([result[2] for result in result_list])\n",
    "    cats_dict = dict()\n",
    "    for k in cats_subset_indexes:\n",
    "        cats_dict[k] = np.hstack([result_list[i][3][k] for i in range(len(result_list))])\n",
    "    features_dict = dict()\n",
    "    for k in features_subset_indexes:\n",
    "        features_dict[k] = np.vstack([result_list[i][4][k] for i in range(len(result_list))])\n",
    "\n",
    "    # To prevent memory error\n",
    "    del result_list\n",
    "\n",
    "    # Convert the categorical features into dummy variables\n",
    "    categorical_dummy_dfs_list = []\n",
    "    for c in cats_subset_indexes:\n",
    "        categorical_dummy_dfs_list.append(pd.get_dummies(cats_dict[c]).iloc[:,:-1])\n",
    "\n",
    "    # Combine all the numpy arrays containg stock price data and features. (`[data]` must go last since the code expects the dependent variable to be the last column)\n",
    "    model_data = np.hstack(categorical_dummy_dfs_list + [features_dict[f] for f in features_subset_indexes] + [data])\n",
    "    del cats_dict, categorical_dummy_dfs_list, features_dict, data\n",
    "\n",
    "    # Finally, covernt to a df\n",
    "    index = pd.MultiIndex.from_tuples(list(zip(data_dates, data_stocks)), names=[\"dates\", \"stocks\"])\n",
    "    model_data = pd.DataFrame(model_data, index=index)\n",
    "\n",
    "    # Normalize the features and apply PCA\n",
    "    features = model_data.iloc[:, :-1]  \n",
    "    print(f\"Number of features before PCA: {len(features.columns)}\")\n",
    "    target = model_data.iloc[:, -1]  \n",
    "    del model_data\n",
    "    features_normalized = (features - features.mean()) / features.std()\n",
    "    del features\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=0.99)\n",
    "    features_pca = pca.fit_transform(features_normalized)\n",
    "    print(f\"Number of features after PCA: {features_pca.shape[1]}\")\n",
    "    del features_normalized\n",
    "\n",
    "    # Recombine the PCA-transformed features with the target\n",
    "    model_data_pca = pd.DataFrame(features_pca, index=index)\n",
    "    model_data_pca['target'] = target\n",
    "    \n",
    "    return model_data_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual explanation of `number_of_lines` hyperparameters.  \n",
    "- Note that it is the gradient of these line-estimations which is inputted into the model along with features and the current stock price.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dataset mimicking daily stock data.\n",
    "np.random.seed(0)  # For reproducibility\n",
    "x = np.linspace(1, 100, 100)\n",
    "y = np.sin(x / 10) + np.random.normal(0, 0.1, 100) * np.linspace(0.1, 1, 100)\n",
    "\n",
    "lines_counts = [1, 2, 4, 8]\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5), dpi=100)\n",
    "\n",
    "for ax, lines_count in zip(axs, lines_counts):\n",
    "    ax.scatter(x, y, alpha=0.7)\n",
    "    \n",
    "    # Determining indices for splitting the data to create 'lines_count' segments\n",
    "    indices = np.linspace(0, len(x) - 1, lines_count + 1, dtype=int)\n",
    "    \n",
    "    for i in range(len(indices) - 1):\n",
    "        # Extracting segments based on calculated indices\n",
    "        segment_x = x[indices[i]:indices[i+1]]\n",
    "        segment_y = y[indices[i]:indices[i+1]]\n",
    "        \n",
    "        # Fit a simple linear regression (polyfit) for each segment\n",
    "        p = np.polyfit(segment_x, segment_y, 1)\n",
    "        \n",
    "        # Plotting the line\n",
    "        ax.plot(segment_x, np.polyval(p, segment_x), '-', linewidth=4, color='black')\n",
    "    \n",
    "    ax.set_title(f\"{lines_count} Line Estimation\")\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_ylabel('Stock Price')\n",
    "\n",
    "fig.suptitle('Explanation of `number_of_lines` hyperparameter', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_EPOCHS = 50\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function allows us to pre-process and create the data for the models using several different hyperparameters such as `number_of_lines`, `number_of_years_to_consider`, `cats_subset` and `features_subset`. It also allows us to adjust the model and training process with several different hyperparameters such as `layer`, `neuron`, `batch_size` and `technique`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(number_of_lines, number_of_years_to_consider, layer, neuron, batch_size, technique, cat_features_subset, num_features_subset, results = results):\n",
    "    # Get name of next model (models are simply named 1,2,3,... and addition information about the model can be found in \"results\\overview\\model results.csv\")\n",
    "    files = os.listdir(models_path)\n",
    "    numbers = sorted([int(file_name[:-3]) for file_name in files])\n",
    "    try:\n",
    "        model_name =  str(numbers[-1] + 1)\n",
    "    except IndexError:\n",
    "        model_name = \"1\"\n",
    "    \n",
    "    # Get pre-processed model data\n",
    "    model_data = prepare_model_data(cat_features_subset, num_features_subset, number_of_years_to_consider,number_of_lines)\n",
    "\n",
    "    # Split the data into 60% train set, 20% valudation set and 20% test set (the test data will be used in the following notebook.)\n",
    "    x_train, x_temp, y_train, y_temp = train_test_split(model_data.iloc[:,:-1], model_data.iloc[:,-1], test_size=0.4, random_state=42, shuffle=False)\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "    # Normalize the data\n",
    "    normalizer = Normalization(axis=-1, input_shape=[x_train.shape[1]])\n",
    "    normalizer.adapt(x_train.to_numpy())\n",
    "\n",
    "    # Model architecture\n",
    "    model = Sequential()\n",
    "    model.add(normalizer)\n",
    "    for _ in range(layer):\n",
    "        model.add(Dense(neuron, activation='relu', kernel_initializer='he_normal'))\n",
    "        if technique == \"dropout & batch\" or technique == \"batch regularisation\":\n",
    "            model.add(BatchNormalization())\n",
    "        if technique == \"dropout & batch\" or technique == \"dropout layers\":\n",
    "            model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_absolute_error', optimizer= Adam(learning_rate=0.00001), metrics=[RSquare()])\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_cb = ModelCheckpoint(f\"{models_path}/{model_name}.h5\", monitor='val_loss', save_best_only=True,verbose=0, save_weights_only=False, mode = \"min\")\n",
    "    early_stopping_cb = EarlyStopping(monitor = \"val_loss\", patience = PATIENCE, restore_best_weights=True, verbose = 0, min_delta = 0.01, mode = \"min\")\n",
    "    # tensorboard_cb = tf.keras.callbacks.TensorBoard(f\"{root_folder}/{details_folder}/{technique}/logs/{data_name}/{model_name}\")\n",
    "    # reducelr_cb = ReduceLROnPlateau(monitor='val_r_square', factor=0.5, patience=REDUCE_LR_PATIENCE, verbose = 0, mode = \"max\")\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data = (x_val, y_val),\n",
    "        verbose=1,\n",
    "        epochs=MAX_EPOCHS,\n",
    "        callbacks=[checkpoint_cb, early_stopping_cb], # , tensorboard_cb , reducelr_cb\n",
    "        batch_size = batch_size)\n",
    "\n",
    "    # Calculate validation loss and r-squared \n",
    "    model = tf.keras.models.load_model(f\"{models_path}/{model_name}.h5\")\n",
    "    test_predictions = model.predict(x_val, batch_size=batch_size)\n",
    "    val_loss = min(history.history[\"val_loss\"])\n",
    "    R2_score = r2_score(y_val, test_predictions)\n",
    "\n",
    "    # Plot the training process and validation results and save figure\n",
    "    plt.close(\"all\")\n",
    "    features_string = f\"{cat_features_subset + num_features_subset}\".replace(\"'\",\"\").replace(\",\",\"\")\n",
    "    fig = plot(history, y_val, len(results), test_predictions, R2_score)\n",
    "    plt.savefig(\n",
    "        f'{graphs_path}/{model_name}.png',\n",
    "        bbox_inches='tight',\n",
    "        dpi = 300\n",
    "    )\n",
    "\n",
    "    # Add result to `results` dataframe \n",
    "    total_neurons = layer * neuron\n",
    "    new_row = pd.DataFrame([{\"features\": features_string,\n",
    "                            \"lines\": number_of_lines,\n",
    "                            \"years\": number_of_years_to_consider,\n",
    "                            \"layers\": layer,\n",
    "                            \"neurons per layer\": neuron,\n",
    "                            \"total neurons\": total_neurons,\n",
    "                            \"batch size\": batch_size,\n",
    "                            \"technique\": technique,\n",
    "                            \"model name\": model_name,\n",
    "                            \"val_loss\": val_loss,\n",
    "                            \"val_r2\": R2_score}])\n",
    "    results = pd.concat([results, new_row], ignore_index=True)\n",
    "\n",
    "    # Add results to \"model results.csv\"\n",
    "    result_row = [features_string, number_of_lines, number_of_years_to_consider, layer, neuron, total_neurons, batch_size, technique, model_name, val_loss, R2_score]\n",
    "    with open(f\"{root_folder}/{overview_folder}/model results.csv\", \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(result_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sets of hyperparameters to try\n",
    "\n",
    "# Feature selection. Choose economic indicators that aren't heavily correlated based on notebook 2 \n",
    "chosen_economic_indicators = [\n",
    "    \"BCI\", \"CCI\", \"CLI\", \"3 month interbank rate\", \"Construction\", \n",
    "    \"Consumer prices\", \"Manufacturing hourly earnings\", \"Industrial production\", \n",
    "    \"Long-term interest rate\", \"Narrow money\", \n",
    "    \"Car registrations\",  \"Total employment\", \n",
    "]\n",
    "\n",
    "# Settings for the new hyperparameters\n",
    "number_of_lines_options = [5, 20, 50]\n",
    "number_of_years_to_consider_options = [2, 3, 4]\n",
    "layers_options = [2, 4, 6]\n",
    "neuron_options = [16, 32]\n",
    "batch_size_options = [1024]\n",
    "\n",
    "# Initialize the hyperparameters dictionary\n",
    "hyperparameters = {\n",
    "    'number_of_lines': [],\n",
    "    'number_of_years_to_consider': [],\n",
    "    'layer': [],\n",
    "    'neuron': [],\n",
    "    'batch_size': [],\n",
    "    'technique': [],\n",
    "    'cat_features_subset': [],\n",
    "    'num_features_subset': []\n",
    "}\n",
    "\n",
    "# Generate a subset of hyperparameters for demonstration\n",
    "for lines in number_of_lines_options:\n",
    "    for years in number_of_years_to_consider_options:\n",
    "        for layer in layers_options:\n",
    "            for neuron in neuron_options:\n",
    "                for batch_size in batch_size_options:\n",
    "                    hyperparameters['number_of_lines'].append(lines)\n",
    "                    hyperparameters['number_of_years_to_consider'].append(years)\n",
    "                    hyperparameters['layer'].append(layer)\n",
    "                    hyperparameters['neuron'].append(neuron)\n",
    "                    hyperparameters['batch_size'].append(batch_size)\n",
    "                    hyperparameters['technique'].append('base')\n",
    "                    hyperparameters['cat_features_subset'].append([\"sectors\"])\n",
    "                    hyperparameters['num_features_subset'].append(chosen_economic_indicators)\n",
    "\n",
    "# We also need base cases to compare to where we don't use any economic indicators\n",
    "for lines in number_of_lines_options:\n",
    "    for years in number_of_years_to_consider_options:\n",
    "        for layer in layers_options:\n",
    "            for neuron in neuron_options:\n",
    "                for batch_size in batch_size_options:\n",
    "                    hyperparameters['number_of_lines'].append(lines)\n",
    "                    hyperparameters['number_of_years_to_consider'].append(years)\n",
    "                    hyperparameters['layer'].append(layer)\n",
    "                    hyperparameters['neuron'].append(neuron)\n",
    "                    hyperparameters['batch_size'].append(batch_size)\n",
    "                    hyperparameters['technique'].append('base')\n",
    "                    hyperparameters['cat_features_subset'].append([])\n",
    "                    hyperparameters['num_features_subset'].append([])\n",
    "\n",
    "# Convert dictionary to df\n",
    "hyperparameters_df = pd.DataFrame.from_dict(hyperparameters, orient='index').transpose()\n",
    "hyperparameters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model using each set of hyperparmeters in `hypterparameters_df`\n",
    "for i in range(len(hyperparameters_df)):\n",
    "    hyperparameters = hyperparameters_df.loc[i+52]\n",
    "    create_model(*hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
