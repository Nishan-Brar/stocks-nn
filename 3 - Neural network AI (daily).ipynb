{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT ABOUT THE SUBTRACTING A YEAR FROM ALL THE FEATURES DATES!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import decimal\n",
    "disable_progress_bars = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If cannot connect to local hosot, run both of the following commands in cmd using the code block below:\n",
    "* `taskkill /im tensorboard.exe /f`\n",
    "* `del /q %TMP%\\.tensorboard-info\\*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.system('cmd /k \"taskkill /im tensorboard.exe /f\"')\n",
    "# os.system('cmd /k \"del /q %TMP%\\.tensorboard-info\\*\"')\n",
    "# \n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir results --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:38.355211Z",
     "start_time": "2023-01-05T12:27:38.341552Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip uninstall -y tensorflow\n",
    "# ! pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:47.124593Z",
     "start_time": "2023-01-05T12:27:38.357356Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from numba import jit\n",
    "from multiprocessing import Pool\n",
    "from outer_function_daily import outer_function\n",
    "from rois import rois_func, portfolio_weights\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import fredapi as fa\n",
    "fred = fa.Fred(api_key=\"5b3325018adf679644bc330c41598801\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import csv\n",
    "import random\n",
    "import datetime\n",
    "import networkx as nx\n",
    "\n",
    "import yfinance as yf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:47.153857Z",
     "start_time": "2023-01-05T12:27:47.124593Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if it detects GPU\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:52.288577Z",
     "start_time": "2023-01-05T12:27:47.211965Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>BRK.B</th>\n",
       "      <th>UNH</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>XOM</th>\n",
       "      <th>V</th>\n",
       "      <th>...</th>\n",
       "      <th>UPST</th>\n",
       "      <th>TAP</th>\n",
       "      <th>WE</th>\n",
       "      <th>CVNA</th>\n",
       "      <th>NVAX</th>\n",
       "      <th>EHAB</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>UWMC</th>\n",
       "      <th>MBC</th>\n",
       "      <th>FG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.291009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.264886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.888242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.963844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.488728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.266664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.895060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.998913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.510702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.267553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.891651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.998913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.378882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.268442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.871201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.998913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.378882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.270220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.880794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.998913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>147.929993</td>\n",
       "      <td>44.090000</td>\n",
       "      <td>12.53</td>\n",
       "      <td>139.380005</td>\n",
       "      <td>132.300003</td>\n",
       "      <td>160.580002</td>\n",
       "      <td>167.380005</td>\n",
       "      <td>87.620003</td>\n",
       "      <td>106.489998</td>\n",
       "      <td>62.240002</td>\n",
       "      <td>...</td>\n",
       "      <td>128.589996</td>\n",
       "      <td>33.619999</td>\n",
       "      <td>125.192581</td>\n",
       "      <td>247.149994</td>\n",
       "      <td>32.669998</td>\n",
       "      <td>28.730000</td>\n",
       "      <td>46.130001</td>\n",
       "      <td>68.559998</td>\n",
       "      <td>113.540001</td>\n",
       "      <td>142.470001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-21</th>\n",
       "      <td>150.089996</td>\n",
       "      <td>44.869999</td>\n",
       "      <td>13.03</td>\n",
       "      <td>142.289993</td>\n",
       "      <td>135.449997</td>\n",
       "      <td>162.210007</td>\n",
       "      <td>169.320007</td>\n",
       "      <td>87.070000</td>\n",
       "      <td>108.139999</td>\n",
       "      <td>63.360001</td>\n",
       "      <td>...</td>\n",
       "      <td>128.779999</td>\n",
       "      <td>33.950001</td>\n",
       "      <td>126.839455</td>\n",
       "      <td>250.839996</td>\n",
       "      <td>33.049999</td>\n",
       "      <td>29.309999</td>\n",
       "      <td>47.570000</td>\n",
       "      <td>69.930000</td>\n",
       "      <td>112.769997</td>\n",
       "      <td>144.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-22</th>\n",
       "      <td>149.009995</td>\n",
       "      <td>43.849998</td>\n",
       "      <td>12.56</td>\n",
       "      <td>142.080002</td>\n",
       "      <td>132.229996</td>\n",
       "      <td>163.270004</td>\n",
       "      <td>169.270004</td>\n",
       "      <td>84.870003</td>\n",
       "      <td>108.029999</td>\n",
       "      <td>62.849998</td>\n",
       "      <td>...</td>\n",
       "      <td>128.820007</td>\n",
       "      <td>33.459999</td>\n",
       "      <td>126.550003</td>\n",
       "      <td>247.509995</td>\n",
       "      <td>32.439999</td>\n",
       "      <td>28.950001</td>\n",
       "      <td>48.279999</td>\n",
       "      <td>65.940002</td>\n",
       "      <td>109.360001</td>\n",
       "      <td>145.029999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>149.229996</td>\n",
       "      <td>44.200001</td>\n",
       "      <td>12.71</td>\n",
       "      <td>143.279999</td>\n",
       "      <td>131.860001</td>\n",
       "      <td>163.100006</td>\n",
       "      <td>170.009995</td>\n",
       "      <td>85.250000</td>\n",
       "      <td>108.180000</td>\n",
       "      <td>63.380001</td>\n",
       "      <td>...</td>\n",
       "      <td>128.899994</td>\n",
       "      <td>33.130001</td>\n",
       "      <td>126.690002</td>\n",
       "      <td>248.220001</td>\n",
       "      <td>32.119999</td>\n",
       "      <td>28.799999</td>\n",
       "      <td>48.450001</td>\n",
       "      <td>65.889999</td>\n",
       "      <td>108.779999</td>\n",
       "      <td>145.759995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>149.550003</td>\n",
       "      <td>44.860001</td>\n",
       "      <td>12.53</td>\n",
       "      <td>145.020004</td>\n",
       "      <td>130.029999</td>\n",
       "      <td>162.990005</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>83.489998</td>\n",
       "      <td>108.570000</td>\n",
       "      <td>63.619999</td>\n",
       "      <td>...</td>\n",
       "      <td>129.899994</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>127.279999</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>31.540001</td>\n",
       "      <td>28.549999</td>\n",
       "      <td>48.840000</td>\n",
       "      <td>65.459999</td>\n",
       "      <td>107.570000</td>\n",
       "      <td>145.300003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8312 rows × 1005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AAPL       MSFT  GOOGL        GOOG        AMZN       BRK.B  \\\n",
       "Date                                                                           \n",
       "1990-01-02         NaN  13.291009    NaN         NaN    0.264886         NaN   \n",
       "1990-01-03         NaN  13.488728    NaN         NaN    0.266664         NaN   \n",
       "1990-01-04         NaN  13.510702    NaN         NaN    0.267553         NaN   \n",
       "1990-01-05         NaN  13.378882    NaN         NaN    0.268442         NaN   \n",
       "1990-01-08         NaN  13.378882    NaN         NaN    0.270220         NaN   \n",
       "...                ...        ...    ...         ...         ...         ...   \n",
       "2022-12-20  147.929993  44.090000  12.53  139.380005  132.300003  160.580002   \n",
       "2022-12-21  150.089996  44.869999  13.03  142.289993  135.449997  162.210007   \n",
       "2022-12-22  149.009995  43.849998  12.56  142.080002  132.229996  163.270004   \n",
       "2022-12-23  149.229996  44.200001  12.71  143.279999  131.860001  163.100006   \n",
       "2022-12-27  149.550003  44.860001  12.53  145.020004  130.029999  162.990005   \n",
       "\n",
       "                   UNH        JNJ         XOM          V  ...        UPST  \\\n",
       "Date                                                      ...               \n",
       "1990-01-02         NaN        NaN    1.888242        NaN  ...         NaN   \n",
       "1990-01-03         NaN        NaN    1.895060        NaN  ...         NaN   \n",
       "1990-01-04         NaN        NaN    1.891651        NaN  ...         NaN   \n",
       "1990-01-05         NaN        NaN    1.871201        NaN  ...         NaN   \n",
       "1990-01-08         NaN        NaN    1.880794        NaN  ...         NaN   \n",
       "...                ...        ...         ...        ...  ...         ...   \n",
       "2022-12-20  167.380005  87.620003  106.489998  62.240002  ...  128.589996   \n",
       "2022-12-21  169.320007  87.070000  108.139999  63.360001  ...  128.779999   \n",
       "2022-12-22  169.270004  84.870003  108.029999  62.849998  ...  128.820007   \n",
       "2022-12-23  170.009995  85.250000  108.180000  63.380001  ...  128.899994   \n",
       "2022-12-27  169.000000  83.489998  108.570000  63.619999  ...  129.899994   \n",
       "\n",
       "                  TAP          WE        CVNA       NVAX       EHAB  \\\n",
       "Date                                                                  \n",
       "1990-01-02        NaN         NaN         NaN        NaN        NaN   \n",
       "1990-01-03        NaN         NaN         NaN        NaN        NaN   \n",
       "1990-01-04        NaN         NaN         NaN        NaN        NaN   \n",
       "1990-01-05        NaN         NaN         NaN        NaN        NaN   \n",
       "1990-01-08        NaN         NaN         NaN        NaN        NaN   \n",
       "...               ...         ...         ...        ...        ...   \n",
       "2022-12-20  33.619999  125.192581  247.149994  32.669998  28.730000   \n",
       "2022-12-21  33.950001  126.839455  250.839996  33.049999  29.309999   \n",
       "2022-12-22  33.459999  126.550003  247.509995  32.439999  28.950001   \n",
       "2022-12-23  33.130001  126.690002  248.220001  32.119999  28.799999   \n",
       "2022-12-27  32.500000  127.279999  251.000000  31.540001  28.549999   \n",
       "\n",
       "                 OPEN       UWMC         MBC          FG  \n",
       "Date                                                      \n",
       "1990-01-02   1.963844        NaN         NaN         NaN  \n",
       "1990-01-03   1.998913        NaN         NaN         NaN  \n",
       "1990-01-04   1.998913        NaN         NaN         NaN  \n",
       "1990-01-05   1.998913        NaN         NaN         NaN  \n",
       "1990-01-08   1.998913        NaN         NaN         NaN  \n",
       "...               ...        ...         ...         ...  \n",
       "2022-12-20  46.130001  68.559998  113.540001  142.470001  \n",
       "2022-12-21  47.570000  69.930000  112.769997  144.919998  \n",
       "2022-12-22  48.279999  65.940002  109.360001  145.029999  \n",
       "2022-12-23  48.450001  65.889999  108.779999  145.759995  \n",
       "2022-12-27  48.840000  65.459999  107.570000  145.300003  \n",
       "\n",
       "[8312 rows x 1005 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "stocks_df = pd.read_csv(\"data/price.csv\")\n",
    "\n",
    "# Set date-time index\n",
    "stocks_df['Date'] = pd.to_datetime(stocks_df[\"Date\"], format='%Y/%m/%d')\n",
    "stocks_df = stocks_df.set_index(stocks_df['Date'])\n",
    "stocks_df = stocks_df.drop(columns = [\"Date\"])\n",
    "\n",
    "# Drop all columns which only contain nan values\n",
    "boolean = []\n",
    "for stock in stocks_df.columns:\n",
    "    boolean.append(not stocks_df[stock].isnull().all())\n",
    "stocks_df = stocks_df.iloc[:, boolean]\n",
    "\n",
    "describe = stocks_df.describe()\n",
    "stocks_df = stocks_df[describe.loc[\"min\",:][describe.loc[\"min\",:]>=0].index]\n",
    "\n",
    "stocks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.628143Z",
     "start_time": "2023-01-05T12:27:54.615358Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "first_date = stocks_df.index[0]\n",
    "last_date = stocks_df.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.865770Z",
     "start_time": "2023-01-05T12:27:54.642845Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JBL</th>\n",
       "      <th>HUN</th>\n",
       "      <th>BSY</th>\n",
       "      <th>LEA</th>\n",
       "      <th>TYL</th>\n",
       "      <th>SCI</th>\n",
       "      <th>META</th>\n",
       "      <th>DCI</th>\n",
       "      <th>APD</th>\n",
       "      <th>CVX</th>\n",
       "      <th>...</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>AEP</th>\n",
       "      <th>M</th>\n",
       "      <th>ARMK</th>\n",
       "      <th>COUP</th>\n",
       "      <th>AWK</th>\n",
       "      <th>INGR</th>\n",
       "      <th>PAG</th>\n",
       "      <th>NCNO</th>\n",
       "      <th>DEI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sectors</th>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Materials</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Materials</td>\n",
       "      <td>Energy</td>\n",
       "      <td>...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Consumer Staples</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Real Estate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            JBL        HUN                     BSY  \\\n",
       "sectors  Information Technology  Materials  Information Technology   \n",
       "\n",
       "                            LEA                     TYL  \\\n",
       "sectors  Consumer Discretionary  Information Technology   \n",
       "\n",
       "                            SCI                    META          DCI  \\\n",
       "sectors  Consumer Discretionary  Communication Services  Industrials   \n",
       "\n",
       "               APD     CVX  ...                    NVDA        AEP  \\\n",
       "sectors  Materials  Energy  ...  Information Technology  Utilities   \n",
       "\n",
       "                              M                    ARMK  \\\n",
       "sectors  Consumer Discretionary  Consumer Discretionary   \n",
       "\n",
       "                           COUP        AWK              INGR  \\\n",
       "sectors  Information Technology  Utilities  Consumer Staples   \n",
       "\n",
       "                            PAG                    NCNO          DEI  \n",
       "sectors  Consumer Discretionary  Information Technology  Real Estate  \n",
       "\n",
       "[1 rows x 1005 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_variables = pd.read_csv(\"data/categorical.csv\")\n",
    "cat_variables = cat_variables.set_index(\"Unnamed: 0\")\n",
    "cat_variables = cat_variables.rename_axis(None, axis = 0)\n",
    "cat_variables = cat_variables.dropna(axis=\"columns\")\n",
    "\n",
    "# Drop stocks such that the remaining stocks are those in both cat_variables and stocks_df\n",
    "intersection = list(set(stocks_df.columns) & set(cat_variables.columns))\n",
    "cat_variables = cat_variables[intersection]\n",
    "stocks_df = stocks_df[intersection]\n",
    "\n",
    "cat_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.881881Z",
     "start_time": "2023-01-05T12:27:54.868869Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Information Technology', 'Materials', 'Information Technology',\n",
       "        ..., 'Consumer Discretionary', 'Information Technology',\n",
       "        'Real Estate']], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_variables_np = cat_variables.to_numpy()\n",
    "cat_variables_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.897130Z",
     "start_time": "2023-01-05T12:27:54.883312Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Communication Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consumer Discretionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consumer Staples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Financials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Industrials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Information Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Information technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Real Estate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Utilities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sectors\n",
       "0   Communication Services\n",
       "1   Consumer Discretionary\n",
       "2         Consumer Staples\n",
       "3                   Energy\n",
       "4               Financials\n",
       "5              Health Care\n",
       "6              Industrials\n",
       "7   Information Technology\n",
       "8   Information technology\n",
       "9                Materials\n",
       "10             Real Estate\n",
       "11               Utilities"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting categories for each categorical variable\n",
    "################## THIS MIGHT GET AN ERROR AS YOU ADD MORE CATEGORICAL VARIABLES SINCE THE COLUMNS BEING ADDED ARE DIFFERENT LENGTHS BUT\n",
    "################## IDK IF IT WILL AUTOMATICALLY REPLACE WITH NAN VALUES !!!!!!!!!!!!\n",
    "cats_df = pd.DataFrame()\n",
    "for cat in cat_variables.index:\n",
    "    uniques = list(cat_variables.loc[cat, :].unique())\n",
    "    categories = np.sort(uniques)\n",
    "    cats_df[cat] = categories\n",
    "\n",
    "cats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.040653Z",
     "start_time": "2023-01-05T12:27:54.970478Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BCI', 'CCI', 'CLI', '3 month interbank rate', 'Broad money',\n",
      "       'Construction', 'Consumer prices', 'Manufacturing hourly earnings',\n",
      "       'Industrial production', 'Long-term interest rate',\n",
      "       'Manufacturing confidence indicator', 'Narrow money',\n",
      "       'Overnight interbank rate', 'Car registrations',\n",
      "       'Manufacturing producer prices', 'Retail trade volume',\n",
      "       'Total employment', 'Total manufacturing', 'Yield curve'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BCI</th>\n",
       "      <th>CCI</th>\n",
       "      <th>CLI</th>\n",
       "      <th>3m %</th>\n",
       "      <th>Broad $</th>\n",
       "      <th>Construction</th>\n",
       "      <th>Consumer prices</th>\n",
       "      <th>Manu $</th>\n",
       "      <th>Indu production</th>\n",
       "      <th>Long %</th>\n",
       "      <th>MCI</th>\n",
       "      <th>Narrow $</th>\n",
       "      <th>Overnight %</th>\n",
       "      <th>Cars</th>\n",
       "      <th>producer $</th>\n",
       "      <th>Retail volume</th>\n",
       "      <th>Total employment</th>\n",
       "      <th>Total manu</th>\n",
       "      <th>Yield curve</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-01-02</th>\n",
       "      <td>98.79037</td>\n",
       "      <td>100.63070</td>\n",
       "      <td>99.78207</td>\n",
       "      <td>8.16</td>\n",
       "      <td>26.291153</td>\n",
       "      <td>40.382780</td>\n",
       "      <td>53.751419</td>\n",
       "      <td>52.853255</td>\n",
       "      <td>61.258212</td>\n",
       "      <td>8.21</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>26.322568</td>\n",
       "      <td>8.23</td>\n",
       "      <td>138.407768</td>\n",
       "      <td>60.738345</td>\n",
       "      <td>67.509220</td>\n",
       "      <td>119081.0</td>\n",
       "      <td>61.432632</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-03</th>\n",
       "      <td>98.79037</td>\n",
       "      <td>100.63070</td>\n",
       "      <td>99.78207</td>\n",
       "      <td>8.16</td>\n",
       "      <td>26.291153</td>\n",
       "      <td>40.382780</td>\n",
       "      <td>53.751419</td>\n",
       "      <td>52.853255</td>\n",
       "      <td>61.258212</td>\n",
       "      <td>8.21</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>26.322568</td>\n",
       "      <td>8.23</td>\n",
       "      <td>138.407768</td>\n",
       "      <td>60.738345</td>\n",
       "      <td>67.509220</td>\n",
       "      <td>119081.0</td>\n",
       "      <td>61.432632</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-04</th>\n",
       "      <td>98.79037</td>\n",
       "      <td>100.63070</td>\n",
       "      <td>99.78207</td>\n",
       "      <td>8.16</td>\n",
       "      <td>26.291153</td>\n",
       "      <td>40.382780</td>\n",
       "      <td>53.751419</td>\n",
       "      <td>52.853255</td>\n",
       "      <td>61.258212</td>\n",
       "      <td>8.21</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>26.322568</td>\n",
       "      <td>8.23</td>\n",
       "      <td>138.407768</td>\n",
       "      <td>60.738345</td>\n",
       "      <td>67.509220</td>\n",
       "      <td>119081.0</td>\n",
       "      <td>61.432632</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-05</th>\n",
       "      <td>98.79037</td>\n",
       "      <td>100.63070</td>\n",
       "      <td>99.78207</td>\n",
       "      <td>8.16</td>\n",
       "      <td>26.291153</td>\n",
       "      <td>40.382780</td>\n",
       "      <td>53.751419</td>\n",
       "      <td>52.853255</td>\n",
       "      <td>61.258212</td>\n",
       "      <td>8.21</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>26.322568</td>\n",
       "      <td>8.23</td>\n",
       "      <td>138.407768</td>\n",
       "      <td>60.738345</td>\n",
       "      <td>67.509220</td>\n",
       "      <td>119081.0</td>\n",
       "      <td>61.432632</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-08</th>\n",
       "      <td>98.79037</td>\n",
       "      <td>100.63070</td>\n",
       "      <td>99.78207</td>\n",
       "      <td>8.16</td>\n",
       "      <td>26.291153</td>\n",
       "      <td>40.382780</td>\n",
       "      <td>53.751419</td>\n",
       "      <td>52.853255</td>\n",
       "      <td>61.258212</td>\n",
       "      <td>8.21</td>\n",
       "      <td>-5.6</td>\n",
       "      <td>26.322568</td>\n",
       "      <td>8.23</td>\n",
       "      <td>138.407768</td>\n",
       "      <td>60.738345</td>\n",
       "      <td>67.509220</td>\n",
       "      <td>119081.0</td>\n",
       "      <td>61.432632</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>99.12707</td>\n",
       "      <td>96.56286</td>\n",
       "      <td>98.44179</td>\n",
       "      <td>4.46</td>\n",
       "      <td>177.791553</td>\n",
       "      <td>162.132691</td>\n",
       "      <td>125.607446</td>\n",
       "      <td>128.465564</td>\n",
       "      <td>103.637942</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>665.175988</td>\n",
       "      <td>3.78</td>\n",
       "      <td>41.423058</td>\n",
       "      <td>136.293182</td>\n",
       "      <td>121.047763</td>\n",
       "      <td>158470.0</td>\n",
       "      <td>101.417601</td>\n",
       "      <td>-0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-21</th>\n",
       "      <td>99.12707</td>\n",
       "      <td>96.56286</td>\n",
       "      <td>98.44179</td>\n",
       "      <td>4.46</td>\n",
       "      <td>177.791553</td>\n",
       "      <td>162.132691</td>\n",
       "      <td>125.607446</td>\n",
       "      <td>128.465564</td>\n",
       "      <td>103.637942</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>665.175988</td>\n",
       "      <td>3.78</td>\n",
       "      <td>41.423058</td>\n",
       "      <td>136.293182</td>\n",
       "      <td>121.047763</td>\n",
       "      <td>158470.0</td>\n",
       "      <td>101.417601</td>\n",
       "      <td>-0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-22</th>\n",
       "      <td>99.12707</td>\n",
       "      <td>96.56286</td>\n",
       "      <td>98.44179</td>\n",
       "      <td>4.46</td>\n",
       "      <td>177.791553</td>\n",
       "      <td>162.132691</td>\n",
       "      <td>125.607446</td>\n",
       "      <td>128.465564</td>\n",
       "      <td>103.637942</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>665.175988</td>\n",
       "      <td>3.78</td>\n",
       "      <td>41.423058</td>\n",
       "      <td>136.293182</td>\n",
       "      <td>121.047763</td>\n",
       "      <td>158470.0</td>\n",
       "      <td>101.417601</td>\n",
       "      <td>-0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>99.12707</td>\n",
       "      <td>96.56286</td>\n",
       "      <td>98.44179</td>\n",
       "      <td>4.46</td>\n",
       "      <td>177.791553</td>\n",
       "      <td>162.132691</td>\n",
       "      <td>125.607446</td>\n",
       "      <td>128.465564</td>\n",
       "      <td>103.637942</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>665.175988</td>\n",
       "      <td>3.78</td>\n",
       "      <td>41.423058</td>\n",
       "      <td>136.293182</td>\n",
       "      <td>121.047763</td>\n",
       "      <td>158470.0</td>\n",
       "      <td>101.417601</td>\n",
       "      <td>-0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>99.12707</td>\n",
       "      <td>96.56286</td>\n",
       "      <td>98.44179</td>\n",
       "      <td>4.46</td>\n",
       "      <td>177.791553</td>\n",
       "      <td>162.132691</td>\n",
       "      <td>125.607446</td>\n",
       "      <td>128.465564</td>\n",
       "      <td>103.637942</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>665.175988</td>\n",
       "      <td>3.78</td>\n",
       "      <td>41.423058</td>\n",
       "      <td>136.293182</td>\n",
       "      <td>121.047763</td>\n",
       "      <td>158470.0</td>\n",
       "      <td>101.417601</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8312 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 BCI        CCI       CLI  3m %     Broad $  Construction  \\\n",
       "Date                                                                        \n",
       "1990-01-02  98.79037  100.63070  99.78207  8.16   26.291153     40.382780   \n",
       "1990-01-03  98.79037  100.63070  99.78207  8.16   26.291153     40.382780   \n",
       "1990-01-04  98.79037  100.63070  99.78207  8.16   26.291153     40.382780   \n",
       "1990-01-05  98.79037  100.63070  99.78207  8.16   26.291153     40.382780   \n",
       "1990-01-08  98.79037  100.63070  99.78207  8.16   26.291153     40.382780   \n",
       "...              ...        ...       ...   ...         ...           ...   \n",
       "2022-12-20  99.12707   96.56286  98.44179  4.46  177.791553    162.132691   \n",
       "2022-12-21  99.12707   96.56286  98.44179  4.46  177.791553    162.132691   \n",
       "2022-12-22  99.12707   96.56286  98.44179  4.46  177.791553    162.132691   \n",
       "2022-12-23  99.12707   96.56286  98.44179  4.46  177.791553    162.132691   \n",
       "2022-12-27  99.12707   96.56286  98.44179  4.46  177.791553    162.132691   \n",
       "\n",
       "            Consumer prices      Manu $  Indu production  Long %  MCI  \\\n",
       "Date                                                                    \n",
       "1990-01-02        53.751419   52.853255        61.258212    8.21 -5.6   \n",
       "1990-01-03        53.751419   52.853255        61.258212    8.21 -5.6   \n",
       "1990-01-04        53.751419   52.853255        61.258212    8.21 -5.6   \n",
       "1990-01-05        53.751419   52.853255        61.258212    8.21 -5.6   \n",
       "1990-01-08        53.751419   52.853255        61.258212    8.21 -5.6   \n",
       "...                     ...         ...              ...     ...  ...   \n",
       "2022-12-20       125.607446  128.465564       103.637942    3.89 -1.0   \n",
       "2022-12-21       125.607446  128.465564       103.637942    3.89 -1.0   \n",
       "2022-12-22       125.607446  128.465564       103.637942    3.89 -1.0   \n",
       "2022-12-23       125.607446  128.465564       103.637942    3.89 -1.0   \n",
       "2022-12-27       125.607446  128.465564       103.637942    3.89 -1.0   \n",
       "\n",
       "              Narrow $  Overnight %        Cars  producer $  Retail volume  \\\n",
       "Date                                                                         \n",
       "1990-01-02   26.322568         8.23  138.407768   60.738345      67.509220   \n",
       "1990-01-03   26.322568         8.23  138.407768   60.738345      67.509220   \n",
       "1990-01-04   26.322568         8.23  138.407768   60.738345      67.509220   \n",
       "1990-01-05   26.322568         8.23  138.407768   60.738345      67.509220   \n",
       "1990-01-08   26.322568         8.23  138.407768   60.738345      67.509220   \n",
       "...                ...          ...         ...         ...            ...   \n",
       "2022-12-20  665.175988         3.78   41.423058  136.293182     121.047763   \n",
       "2022-12-21  665.175988         3.78   41.423058  136.293182     121.047763   \n",
       "2022-12-22  665.175988         3.78   41.423058  136.293182     121.047763   \n",
       "2022-12-23  665.175988         3.78   41.423058  136.293182     121.047763   \n",
       "2022-12-27  665.175988         3.78   41.423058  136.293182     121.047763   \n",
       "\n",
       "            Total employment  Total manu  Yield curve  \n",
       "Date                                                   \n",
       "1990-01-02          119081.0   61.432632         0.11  \n",
       "1990-01-03          119081.0   61.432632         0.10  \n",
       "1990-01-04          119081.0   61.432632         0.14  \n",
       "1990-01-05          119081.0   61.432632         0.20  \n",
       "1990-01-08          119081.0   61.432632         0.23  \n",
       "...                      ...         ...          ...  \n",
       "2022-12-20          158470.0  101.417601        -0.66  \n",
       "2022-12-21          158470.0  101.417601        -0.65  \n",
       "2022-12-22          158470.0  101.417601        -0.68  \n",
       "2022-12-23          158470.0  101.417601        -0.59  \n",
       "2022-12-27          158470.0  101.417601        -0.62  \n",
       "\n",
       "[8312 rows x 19 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_daily = pd.read_csv(\"data/features.csv\")\n",
    "features_daily['Date'] = pd.to_datetime(features_daily[\"Date\"], format='%Y/%m/%d')\n",
    "features_daily = features_daily.set_index(\"Date\")\n",
    "features_daily = features_daily.dropna()\n",
    "\n",
    "# Make names smaller to use in file names\n",
    "print(features_daily.columns)\n",
    "features_daily.columns = ['BCI', 'CCI', 'CLI', '3m %', 'Broad $', 'Construction', 'Consumer prices', 'Manu $',\n",
    "       'Indu production', 'Long %', 'MCI', 'Narrow $', 'Overnight %', 'Cars', 'producer $', 'Retail volume',\n",
    "       'Total employment', 'Total manu', 'Yield curve']\n",
    "\n",
    "features_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.056594Z",
     "start_time": "2023-01-05T12:27:55.043690Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.87903700e+01,  1.00630700e+02,  9.97820700e+01, ...,\n",
       "         1.19081000e+05,  6.14326323e+01,  1.10000000e-01],\n",
       "       [ 9.87903700e+01,  1.00630700e+02,  9.97820700e+01, ...,\n",
       "         1.19081000e+05,  6.14326323e+01,  1.00000000e-01],\n",
       "       [ 9.87903700e+01,  1.00630700e+02,  9.97820700e+01, ...,\n",
       "         1.19081000e+05,  6.14326323e+01,  1.40000000e-01],\n",
       "       ...,\n",
       "       [ 9.91270700e+01,  9.65628600e+01,  9.84417900e+01, ...,\n",
       "         1.58470000e+05,  1.01417601e+02, -6.80000000e-01],\n",
       "       [ 9.91270700e+01,  9.65628600e+01,  9.84417900e+01, ...,\n",
       "         1.58470000e+05,  1.01417601e+02, -5.90000000e-01],\n",
       "       [ 9.91270700e+01,  9.65628600e+01,  9.84417900e+01, ...,\n",
       "         1.58470000e+05,  1.01417601e+02, -6.20000000e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_daily_np = features_daily.to_numpy()\n",
    "features_daily_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.072276Z",
     "start_time": "2023-01-05T12:27:55.058689Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_feature_index(feature):\n",
    "    \"\"\"\n",
    "    Returns the index in the numpy feature array using the features name.\n",
    "    \"\"\"\n",
    "    return list(features_daily.columns).index(feature)\n",
    "\n",
    "def get_cat_index(cat):\n",
    "    \"\"\"\n",
    "    Returns the index in the numpy feature array using the features name.\n",
    "    \"\"\"\n",
    "    return list(cat_variables.index).index(cat)\n",
    "\n",
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.098213Z",
     "start_time": "2023-01-05T12:27:55.076423Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_EPOCHS = 200\n",
    "PATIENCE = 15\n",
    "NUMBER_OF_RATES = 3\n",
    "\n",
    "# Neural network hyperparameters to try\n",
    "layers = [2, 3, 4]\n",
    "neurons = [32, 64, 128]\n",
    "\n",
    "# Techniques\n",
    "techniques = [\"base\", \"dropout layers\", \"batch regularisation\", \"dropout & batch\"]\n",
    "\n",
    "# Hyperparameters\n",
    "years = [3, 4, 5]\n",
    "lines = [16, 32,64]\n",
    "batch_sizes = [2048]\n",
    "# The categorical and numeric features included in the data are the ones at the SAME INDEX !!!\n",
    "cats_subsets = [[\"sectors\"]]\n",
    "features_subsets = [[\"Manu $\", \"Narrow $\", \"Cars\", \"Indu production\"]]\n",
    "\n",
    "\n",
    "if len(cats_subsets) != len(features_subsets):\n",
    "    raise ValueError(\"The length of `cats_subsets` & `features_subsets` must be the same since the features included in the model \"\n",
    "                     \"are those at the same indexes of these lists.\")\n",
    "all_cats = set()\n",
    "for subset in cats_subsets:\n",
    "    for var in subset:\n",
    "        all_cats.add(var)\n",
    "all_features = set()\n",
    "for subset in features_subsets:\n",
    "    for var in subset:\n",
    "        all_features.add(var)\n",
    "\n",
    "# Sort for consistency\n",
    "all_cats = sorted(list(all_cats))\n",
    "all_features = sorted(list(all_features))\n",
    "\n",
    "all_cats_indexes = []\n",
    "for var in all_cats:\n",
    "    all_cats_indexes.append(get_cat_index(var))\n",
    "all_features_indexes = []\n",
    "for var in all_features:\n",
    "    all_features_indexes.append(get_feature_index(var))\n",
    "\n",
    "\n",
    "##### FOR `cats_subsets`\n",
    "# Sort the names so that the name of the model files and graphs will always be the same regardless of the order of the names in the subset\n",
    "# which allows the program to skip creating the model as it has already been created.\n",
    "for i in range(len(cats_subsets)):\n",
    "    cats_subsets[i].sort()\n",
    "\n",
    "cats_indexes = []\n",
    "for subset_names in cats_subsets:\n",
    "    subset_indexes = []\n",
    "    for feature in subset_names:\n",
    "        subset_indexes.append(get_cat_index(feature))\n",
    "    cats_indexes.append(subset_indexes)\n",
    "\n",
    "\n",
    "##### FOR `features_subsets`\n",
    "# Sort the names so that the name of the model files and graphs will always be the same regardless of the order of the names in the subset\n",
    "# which allows the program to skip creating the model as it has already been created.\n",
    "for i in range(len(features_subsets)):\n",
    "    features_subsets[i].sort()\n",
    "\n",
    "features_subsets_indexes = []\n",
    "for subset_names in features_subsets:\n",
    "    subset_indexes = []\n",
    "    for feature in subset_names:\n",
    "        subset_indexes.append(get_feature_index(feature))\n",
    "    features_subsets_indexes.append(subset_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural network on created data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.113564Z",
     "start_time": "2023-01-05T12:27:55.100287Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.4737463 ,   1.9219991 ,          nan, ...,          nan,\n",
       "                 nan,          nan],\n",
       "       [ 12.09731579,   1.93364799,          nan, ...,          nan,\n",
       "                 nan,          nan],\n",
       "       [ 11.84789371,   1.95694411,          nan, ...,          nan,\n",
       "                 nan,          nan],\n",
       "       ...,\n",
       "       [ 44.72999954,  49.47000122,  69.91000366, ..., 379.94000244,\n",
       "        165.61000061,  57.79999924],\n",
       "       [ 45.13999939,  49.95999908,  69.93000031, ..., 381.85998535,\n",
       "        164.63999939,  58.11999893],\n",
       "       [ 44.59999847,  50.06000137,  69.02999878, ..., 380.64001465,\n",
       "        163.3999939 ,  57.52000046]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_np = stocks_df.to_numpy()\n",
    "stocks_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Very good explanation of [multiprocessing mapping methods](https://stackoverflow.com/questions/26520781/multiprocessing-pool-whats-the-difference-between-map-async-and-imap).\n",
    "* Good explanation of affect of [optimal chunksize](https://stackoverflow.com/questions/34988692/python-3-multiprocessing-optimal-chunk-size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I THINK THERE MAY STILL BE ROOM FOR IMPROVEMENT FOR IN MULTIPROCESSING BY VARYING CHUNKSIZE IN `imap` METHOD.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.129251Z",
     "start_time": "2023-01-05T12:27:55.115873Z"
    }
   },
   "outputs": [],
   "source": [
    "def graph_already_completed(graph_path, layer, neuron):\n",
    "    completed_graphs = os.listdir(graph_path)\n",
    "    for graph_name in completed_graphs:\n",
    "        if f\"{layer}L-{neuron}N\" in graph_name:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.160287Z",
     "start_time": "2023-01-05T12:27:55.130566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>lines</th>\n",
       "      <th>years</th>\n",
       "      <th>layers</th>\n",
       "      <th>neurons per layer</th>\n",
       "      <th>total neurons</th>\n",
       "      <th>batch size</th>\n",
       "      <th>technique</th>\n",
       "      <th>val_r_square</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [features, lines, years, layers, neurons per layer, total neurons, batch size, technique, val_r_square]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_folder = \"results (daily)\"\n",
    "overview_folder = \"overview\"\n",
    "details_folder = \"details\"\n",
    "\n",
    "# Create/load file to store results of each model\n",
    "try:\n",
    "    os.makedirs(f\"{root_folder}/{overview_folder}\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "\n",
    "model_results_path = f\"{root_folder}/{overview_folder}/model results.csv\"\n",
    "try:\n",
    "    results = pd.read_csv(model_results_path)\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist then we create the file\n",
    "    with open(model_results_path, \"w\") as f:\n",
    "        f.write(\",\".join([\"features\",\"lines\", \"years\", \"layers\", \"neurons per layer\", \"total neurons\", \"batch size\", \"technique\", \"val_r_square\"]) + \"\\n\")\n",
    "    results = pd.read_csv(model_results_path)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.175694Z",
     "start_time": "2023-01-05T12:27:55.161383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 3),\n",
       " (16, 4),\n",
       " (16, 5),\n",
       " (32, 3),\n",
       " (32, 4),\n",
       " (32, 5),\n",
       " (64, 3),\n",
       " (64, 4),\n",
       " (64, 5)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This also ensures that all the hyperparameters sets where no features (other than stock price) are considered always run first as they\n",
    "# act as a base to compare to.\n",
    "base_hyperparameter_combinations = []\n",
    "for NUMBER_OF_LINES in lines:\n",
    "    for NUMBER_OF_YEARS_TO_CONSIDER in years:\n",
    "        tup = (NUMBER_OF_LINES, NUMBER_OF_YEARS_TO_CONSIDER)\n",
    "        base_hyperparameter_combinations.append(tup)\n",
    "\n",
    "base_hyperparameter_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.192106Z",
     "start_time": "2023-01-05T12:27:55.177697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, [13, 8, 7, 11]), (0, [0]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This also ensures that all the hyperparameters sets where no features (other than stock price) are considered always run first as they\n",
    "# act as a base to compare to.\n",
    "feature_combinations = []\n",
    "for (subset_i, features_subset), (subset_j, cats_index) in zip(enumerate(features_subsets_indexes), enumerate(cats_indexes)):\n",
    "    tup = ((subset_i, features_subset), (subset_j, cats_index))\n",
    "    feature_combinations.append(tup)\n",
    "\n",
    "feature_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping rows in results for models to redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.207868Z",
     "start_time": "2023-01-05T12:27:55.194371Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_model_by_features(features_string, lines, years, layers, neurons_per_layer, results):\n",
    "    index = results.loc[(results.features==features_string) &\n",
    "                        (results.lines==lines) &\n",
    "                        (results.years==years) &\n",
    "                        (results.layers==layers) &\n",
    "                        (results[\"neurons per layer\"]==neurons_per_layer),:].index[0]\n",
    "    results = results.drop(index, axis=0)\n",
    "    return results\n",
    "\n",
    "def remove_model_by_index(indexes, results):\n",
    "    for i in indexes:\n",
    "        results = results.drop(i, axis=0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove by features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.223087Z",
     "start_time": "2023-01-05T12:27:55.209943Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Table of models to redo (By features)\n",
    "#\n",
    "# table=[[]]\n",
    "# for m in table:\n",
    "#     features_string, lines, years, layers, neurons_per_layer = m\n",
    "#     results = remove_model_by_features(features_string, lines, years, layers, neurons_per_layer, results)\n",
    "# results.to_csv(\"model results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.238548Z",
     "start_time": "2023-01-05T12:27:55.225333Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # List of indexes to remove to redo the models\n",
    "#\n",
    "# list_of_index = []\n",
    "# results = remove_model_by_index(list_of_index, results)\n",
    "# results.to_csv(\"model results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.253812Z",
     "start_time": "2023-01-05T12:27:55.240604Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot(history, model, x_test, y_train, y_test, features_string, batch_size, i, R2, test_predictions):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13, 5), dpi=300)\n",
    "\n",
    "    # PLot actual vs prediction scatter plot\n",
    "    test_predictions = test_predictions.flatten()\n",
    "    ax[0].scatter(y_test, test_predictions, alpha=0.01)\n",
    "    ax[0].set_xlabel('True Values')\n",
    "    ax[0].set_ylabel('Predictions')\n",
    "\n",
    "    # Plot y=x line\n",
    "    # ax[0].axline((0, 0), slope=1, c='green', linewidth=2)\n",
    "    points = [0, max(max(y_test), max(test_predictions))]\n",
    "    ax[0].plot(points, points, color=\"green\", linewidth = 3)\n",
    "\n",
    "    # Plot the average line\n",
    "    # ax[0].axline((0, np.mean(y_train)), slope=0, c='red', linewidth=2, label='mean')\n",
    "    x = [min(y_test), max(y_test)]\n",
    "    y = [np.mean(y_train), np.mean(y_train)]\n",
    "    ax[0].plot(x, y, color=\"red\", linewidth = 3)\n",
    "    ax[0].set_title(\"True values against predictions\")\n",
    "\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].set_xscale('log')\n",
    "\n",
    "    # Plot training and testing r-squared\n",
    "    ax[1].plot(history.history['r_square'], label='r_square')\n",
    "    ax[1].plot(history.history['val_r_square'], label='val_r_square')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Error')\n",
    "    ax[1].set_title('R-Squared')\n",
    "    ax[1].set_ylim([0, 1])\n",
    "    ax[1].set_yticks(np.arange(0, 1.05, 0.05))\n",
    "    # xticks = np.arange(len(history.history['val_r_square'])) + 1\n",
    "    # ax[1].set_xticks(xticks.astype(int))\n",
    "\n",
    "    fig.suptitle(f\"Test R2={round(R2,3)}, BS={batch_size}, result index={i}, {features_string}\", fontsize=\"small\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.269156Z",
     "start_time": "2023-01-05T12:27:55.255812Z"
    }
   },
   "outputs": [],
   "source": [
    "def models_remaining(layers, neurons, batch_sizes, techniques,feature_combinations, models_to_redo_by_index):\n",
    "    for layer in layers:\n",
    "        for neuron in neurons:\n",
    "            for BATCH_SIZE in batch_sizes:\n",
    "                for technique in techniques:\n",
    "                    for (subset_i, features_subset), (subset_j, cats_index) in feature_combinations:\n",
    "                        features_string = f\"{cats_subsets[subset_j] + features_subsets[subset_i]}\".replace(\"'\",\"\").replace(\",\",\"\")\n",
    "                        row = [features_string, NUMBER_OF_LINES, NUMBER_OF_YEARS_TO_CONSIDER, layer, neuron, layer*neuron, BATCH_SIZE, technique]\n",
    "\n",
    "                        # The following determines if the current `row` index was designated to be trained further. The try-except clause takes into account the case\n",
    "                        # where the `row` doesn't exist in `results` df.\n",
    "                        try:\n",
    "                            idx = (results[results.columns[:-1]] == row).index[0]\n",
    "                        except IndexError:\n",
    "                            boolean = False\n",
    "                        else:\n",
    "                            boolean = idx in models_to_redo_by_index\n",
    "                        # If the model has been trained or needs to be trained more, we return True\n",
    "                        if not (results[results.columns[:-1]] == row).all(1).any() or boolean:\n",
    "                            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.285082Z",
     "start_time": "2023-01-05T12:27:55.269367Z"
    }
   },
   "outputs": [],
   "source": [
    "models_to_redo_by_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T13:00:29.612666Z",
     "start_time": "2023-01-05T12:27:55.287218Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f03210361c944a2b3dc9fa981cfa2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed models:   0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aba79b3da7b465692817fcdd0594dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Working on Dataset: 16 lines, 3 years:   0%|          | 0/6851 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.21104454e-03  2.14603548e-02 -2.77962913e-03 ... -3.34650157e-02\n",
      "   1.33944664e+01  2.37628937e+01]\n",
      " [ 4.91560176e-04  1.25183905e-02 -1.01478353e-02 ... -6.97655257e-03\n",
      "   5.42819309e+00  9.17899895e+00]\n",
      " [-1.00423575e-03 -9.38942191e-04  1.57462430e-03 ...  1.09556619e-03\n",
      "   4.02421331e+00  4.32982016e+00]\n",
      " ...\n",
      " [ 3.56118377e-01  1.62572527e-01  1.48744621e-01 ...  3.86673571e-01\n",
      "   5.58280029e+02  3.81859985e+02]\n",
      " [ 3.31659118e-01  1.22094637e-01 -4.36974949e-02 ... -3.20204773e-01\n",
      "   2.28699997e+02  1.64639999e+02]\n",
      " [-4.24509166e-02 -2.20510596e-02  8.56707696e-02 ... -1.13122975e-01\n",
      "   9.66007462e+01  5.81199989e+01]]\n",
      "\n",
      "Working on model:\n",
      "Data: 16Lines-3Years-[Cars, Indu production, Manu $, Narrow $]Features-[sectors]Categorical\n",
      "Layers: 2\n",
      "Neurons: 32\n",
      "Batch size = 2048\n",
      "Technique = base\n",
      "\n",
      "Epoch 1/200\n",
      "1398/1398 [==============================] - 10s 5ms/step - loss: 88.5219 - r_square: -0.3294 - val_loss: 48.9385 - val_r_square: -0.1386\n",
      "Epoch 2/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 59.6962 - r_square: 0.0671 - val_loss: 29.3907 - val_r_square: 0.2307\n",
      "Epoch 3/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 43.4720 - r_square: 0.3572 - val_loss: 23.8755 - val_r_square: 0.4345\n",
      "Epoch 4/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 34.5223 - r_square: 0.5526 - val_loss: 18.8116 - val_r_square: 0.6147\n",
      "Epoch 5/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 25.9183 - r_square: 0.7222 - val_loss: 14.4007 - val_r_square: 0.7658\n",
      "Epoch 6/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 19.9776 - r_square: 0.8423 - val_loss: 12.4733 - val_r_square: 0.8510\n",
      "Epoch 7/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 17.4910 - r_square: 0.8940 - val_loss: 12.2398 - val_r_square: 0.8776\n",
      "Epoch 8/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.8571 - r_square: 0.9075 - val_loss: 11.9998 - val_r_square: 0.8846\n",
      "Epoch 9/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.6532 - r_square: 0.9104 - val_loss: 11.8432 - val_r_square: 0.8863\n",
      "Epoch 10/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.5726 - r_square: 0.9112 - val_loss: 11.7709 - val_r_square: 0.8870\n",
      "Epoch 11/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.5191 - r_square: 0.9116 - val_loss: 11.7346 - val_r_square: 0.8874\n",
      "Epoch 12/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.4717 - r_square: 0.9118 - val_loss: 11.7015 - val_r_square: 0.8875\n",
      "Epoch 13/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.4364 - r_square: 0.9120 - val_loss: 11.6857 - val_r_square: 0.8876\n",
      "Epoch 14/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.4071 - r_square: 0.9122 - val_loss: 11.6818 - val_r_square: 0.8876\n",
      "Epoch 15/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.3803 - r_square: 0.9123 - val_loss: 11.6728 - val_r_square: 0.8877\n",
      "Epoch 16/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.3554 - r_square: 0.9124 - val_loss: 11.6745 - val_r_square: 0.8876\n",
      "Epoch 17/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.3326 - r_square: 0.9126 - val_loss: 11.6700 - val_r_square: 0.8877\n",
      "Epoch 18/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.3065 - r_square: 0.9127 - val_loss: 11.6581 - val_r_square: 0.8878\n",
      "Epoch 19/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.2813 - r_square: 0.9128 - val_loss: 11.6548 - val_r_square: 0.8878\n",
      "Epoch 20/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.2646 - r_square: 0.9129 - val_loss: 11.6437 - val_r_square: 0.8879\n",
      "Epoch 21/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.2497 - r_square: 0.9130 - val_loss: 11.6380 - val_r_square: 0.8879\n",
      "Epoch 22/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.2357 - r_square: 0.9131 - val_loss: 11.6301 - val_r_square: 0.8879\n",
      "Epoch 23/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.2229 - r_square: 0.9132 - val_loss: 11.6277 - val_r_square: 0.8879\n",
      "Epoch 24/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.2107 - r_square: 0.9133 - val_loss: 11.6315 - val_r_square: 0.8878\n",
      "Epoch 25/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.1990 - r_square: 0.9133 - val_loss: 11.6238 - val_r_square: 0.8879\n",
      "Epoch 26/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.1875 - r_square: 0.9135 - val_loss: 11.6158 - val_r_square: 0.8879\n",
      "Epoch 27/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.1761 - r_square: 0.9135 - val_loss: 11.6206 - val_r_square: 0.8879\n",
      "Epoch 28/200\n",
      "1398/1398 [==============================] - 4s 3ms/step - loss: 16.1646 - r_square: 0.9136 - val_loss: 11.6159 - val_r_square: 0.8880\n",
      "646/646 [==============================] - 1s 1ms/step\n",
      "1398/1398 [==============================] - 2s 1ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5421b2c6e39426785fefb50eb1ea67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Trying different rates:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862a146996b94749b1a220e55b4555f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting rois for rate: 0.333:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041ec4e41ed840128f4a7567ef238737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting rois for rate: 0.667:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0992caab31a43b68d597e99f55ed54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting rois for rate: 1.0:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = Pool(os.cpu_count())\n",
    "pbar = tqdm(total=len(base_hyperparameter_combinations)*len(feature_combinations)*len(layers)*len(neurons)*len(batch_sizes)*len(techniques), desc = \"Completed models\")\n",
    "for NUMBER_OF_LINES, NUMBER_OF_YEARS_TO_CONSIDER in base_hyperparameter_combinations:\n",
    "    if models_remaining(layers, neurons, batch_sizes, techniques,feature_combinations, models_to_redo_by_index):\n",
    "        if NUMBER_OF_LINES <= NUMBER_OF_YEARS_TO_CONSIDER*365:\n",
    "            first_index = NUMBER_OF_YEARS_TO_CONSIDER*365+1 # `+1` needed since slicing is end-exclusive\n",
    "            range_iterations = range(first_index, len(stocks_np) - 365)\n",
    "\n",
    "            iterations = tqdm(range_iterations,\n",
    "                              desc=f\"Working on Dataset: {NUMBER_OF_LINES} lines, {NUMBER_OF_YEARS_TO_CONSIDER} years\",\n",
    "                              disable=False)\n",
    "\n",
    "            result_list = p.map(partial(outer_function,\n",
    "                                        stocks_np = stocks_np,\n",
    "                                        NUMBER_OF_YEARS_TO_CONSIDER = NUMBER_OF_YEARS_TO_CONSIDER,\n",
    "                                        NUMBER_OF_LINES = NUMBER_OF_LINES,\n",
    "                                        features_subset_indexes = all_features_indexes,\n",
    "                                        features_np = features_daily_np,\n",
    "                                        cats_np = cat_variables_np,\n",
    "                                        cats_index = all_cats_indexes,\n",
    "                                        cats_df = cats_df,\n",
    "                                        dates = np.array(stocks_df.index),\n",
    "                                        stocks = np.array(stocks_df.columns)\n",
    "                                        ),\n",
    "                                iterations)\n",
    "\n",
    "            data = np.vstack([result[0] for result in result_list])\n",
    "            # data = data.astype(np.float64) ## I don't think this is needed\n",
    "            print(data)\n",
    "            data_dates = np.hstack([result[1] for result in result_list])\n",
    "            data_stocks = np.hstack([result[2] for result in result_list])\n",
    "\n",
    "            cats_dict = dict()\n",
    "            for k in all_cats_indexes:\n",
    "                cats_dict[k] = np.hstack([result_list[i][3][k] for i in range(len(result_list))])\n",
    "\n",
    "            features_dict = dict()\n",
    "            for k in all_features_indexes:\n",
    "                features_dict[k] = np.vstack([result_list[i][4][k] for i in range(len(result_list))])\n",
    "\n",
    "\n",
    "            # Creating neural networks\n",
    "            for layer in layers:\n",
    "                for neuron in neurons:\n",
    "                    for BATCH_SIZE in batch_sizes:\n",
    "                        for technique in techniques:\n",
    "                            for (subset_i, features_subset), (subset_j, cats_index) in feature_combinations:\n",
    "                                data_name = f\"{NUMBER_OF_LINES}Lines-{NUMBER_OF_YEARS_TO_CONSIDER}Years-{features_subsets[subset_i]}Features-{cats_subsets[subset_j]}Categorical\".replace(\"'\", \"\")\n",
    "                                features_string = f\"{cats_subsets[subset_j] + features_subsets[subset_i]}\".replace(\"'\",\"\").replace(\",\",\"\")\n",
    "                                row = [features_string, NUMBER_OF_LINES, NUMBER_OF_YEARS_TO_CONSIDER, layer, neuron, layer*neuron, BATCH_SIZE, technique]\n",
    "                                # The following determines if the current `row` index was designated to be trained further. The try-except clause takes into account the case\n",
    "                                # where the `row` doesn't exist in `results` df.\n",
    "                                try:\n",
    "                                    idx = (results[results.columns[:-1]] == row).index[0]\n",
    "                                except IndexError:\n",
    "                                    boolean = False\n",
    "                                else:\n",
    "                                    boolean = idx in models_to_redo_by_index\n",
    "                                    # If the model has been trained or needs to be trained more, we return True\n",
    "                                if not (results[results.columns[:-1]] == row).all(1).any() or boolean:\n",
    "\n",
    "                                    categorical_dummy_dfs_list = []\n",
    "                                    for c in cats_index:\n",
    "                                        categorical_dummy_dfs_list.append(pd.get_dummies(cats_dict[c]).iloc[:,:-1])\n",
    "\n",
    "\n",
    "                                    # The `data` list must go last since the rest of the script expects the dependent variable to be the last column !!\n",
    "                                    model_data = np.hstack(categorical_dummy_dfs_list + [features_dict[f] for f in features_subset] + [data])\n",
    "\n",
    "\n",
    "                                    # Create data for model\n",
    "                                    tuples = list(zip(data_dates, data_stocks))\n",
    "                                    index = pd.MultiIndex.from_tuples(tuples, names=[\"dates\", \"stocks\"])\n",
    "                                    model_data = pd.DataFrame(model_data, index=index)\n",
    "\n",
    "#############################################################################################################################################\n",
    "                                    model_data_train = pd.concat([model_data.loc[:\"2006-5-1\",:], model_data.loc[\"2014-5-1\":,:]], axis=\"rows\").sample(frac=1,\n",
    "                                                                                                                                                     random_state=1)\n",
    "                                    model_data_test = model_data.loc[\"2006-5-1\":\"2014-5-1\", :]\n",
    "#############################################################################################################################################\n",
    "                                    x_train = model_data_train.iloc[:,:-1]\n",
    "                                    y_train = model_data_train.iloc[:,-1]\n",
    "\n",
    "                                    x_test = model_data_test.iloc[:,:-1]\n",
    "                                    y_test = model_data_test.iloc[:,-1]\n",
    "#############################################################################################################################################\n",
    "                                    test_dates = model_data_test.index\n",
    "                                    \n",
    "                                    # Create folders\n",
    "                                    graphs_path = f'{root_folder}/{details_folder}/{technique}/models training & testing/{data_name}'\n",
    "                                    try:\n",
    "                                        os.makedirs(graphs_path)\n",
    "                                    except FileExistsError:\n",
    "                                        pass\n",
    "\n",
    "                                    models_path = f'{root_folder}/{details_folder}/{technique}/models/{data_name}'\n",
    "                                    try:\n",
    "                                        os.makedirs(models_path)\n",
    "                                    except FileExistsError:\n",
    "                                        pass\n",
    "\n",
    "                                    portfolio_creation_testing_path = f\"{root_folder}/{details_folder}/{technique}/investment results/testing/{data_name}\"\n",
    "                                    try:\n",
    "                                        os.makedirs(portfolio_creation_testing_path)\n",
    "                                    except FileExistsError:\n",
    "                                        pass\n",
    "\n",
    "                                    portfolio_creation_training_path = f\"{root_folder}/{details_folder}/{technique}/investment results/training/{data_name}\"\n",
    "                                    try:\n",
    "                                        os.makedirs(portfolio_creation_training_path)\n",
    "                                    except FileExistsError:\n",
    "                                        pass\n",
    "\n",
    "                                    model_name = f\"{BATCH_SIZE}BS-{layer}Layers-{neuron}Neurons\"\n",
    "\n",
    "                                    print(f\"\\n\"\n",
    "                                          f\"Working on model:\\n\"\n",
    "                                          f\"Data: {data_name}\\n\"\n",
    "                                          f\"Layers: {layer}\\n\"\n",
    "                                          f\"Neurons: {neuron}\\n\"\n",
    "                                          f\"Batch size = {BATCH_SIZE}\\n\"\n",
    "                                          f\"Technique = {technique}\\n\")\n",
    "\n",
    "                                    # Normalize the data\n",
    "                                    normalizer = preprocessing.Normalization(axis=-1)\n",
    "\n",
    "                                    # The following determines if the current `row` index was designated to be trained further. The try-except clause takes into account the case\n",
    "                                    # where the `row` doesn't exist in `results` df.\n",
    "                                    try:\n",
    "                                        idx = (results[results.columns[:-1]] == row).index[0]\n",
    "                                    except IndexError:\n",
    "                                        boolean = False\n",
    "                                    else:\n",
    "                                        boolean = idx in models_to_redo_by_index\n",
    "                                        # If the model has been trained or needs to be trained more, we return True\n",
    "                                    if not boolean:\n",
    "                                        model = Sequential()\n",
    "                                        model.add(normalizer)\n",
    "                                        if technique == \"dropout & batch\":\n",
    "                                            for _ in range(layer):\n",
    "                                                model.add(Dense(neuron))\n",
    "                                                model.add(BatchNormalization(axis=1))\n",
    "                                                model.add(Activation(\"relu\"))\n",
    "                                                model.add(Dropout(0.5))\n",
    "                                        else:\n",
    "                                            for _ in range(layer):\n",
    "                                                model.add(Dense(neuron))\n",
    "                                                if technique == \"batch regularisation\":\n",
    "                                                    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "                                                model.add(Activation(\"relu\"))\n",
    "\n",
    "                                                if technique == \"dropout layers\":\n",
    "                                                    model.add(Dropout(0.5))\n",
    "                                        model.add(Dense(1))\n",
    "                                    else:\n",
    "                                        model = tf.keras.models.load_model(f\"{models_path}/{model_name}.h5\")\n",
    "\n",
    "                                    model.compile(loss='mean_absolute_error',\n",
    "                                                  optimizer= Adam(learning_rate=0.00001),\n",
    "                                                  metrics=[RSquare()])\n",
    "\n",
    "                                    checkpoint_cb = ModelCheckpoint(f\"{models_path}/{model_name}.h5\",\n",
    "                                                                    monitor='val_r_square',\n",
    "                                                                    save_best_only=True,\n",
    "                                                                    verbose=0,\n",
    "                                                                    save_weights_only=False,\n",
    "                                                                    mode = \"max\")\n",
    "                                    early_stopping_cb = EarlyStopping(monitor = \"val_r_square\",\n",
    "                                                                      patience = PATIENCE, # No. epochs with no improvement after which training is stopped.\n",
    "                                                                      restore_best_weights=True,\n",
    "                                                                      verbose = 0,\n",
    "                                                                      min_delta = 0.01, # Increase of 0.01 per `PATIENCE` iterations required at least\n",
    "                                                                      mode = \"max\")\n",
    "\n",
    "                                    \n",
    "                                    # tensorboard_callback = tf.keras.callbacks.TensorBoard(f\"{root_folder}/{details_folder}/{technique}/logs/{data_name}/{model_name}\")\n",
    "\n",
    "                                    # reducelr_cb = ReduceLROnPlateau(monitor='val_r_square',\n",
    "                                    #                                 factor=0.5,\n",
    "                                    #                                 patience=REDUCE_LR_PATIENCE,\n",
    "                                    #                                 verbose = 0,\n",
    "                                    #                                 mode = \"max\")\n",
    "\n",
    "                                    history = model.fit(\n",
    "                                        x_train, y_train,\n",
    "                                        validation_data = (x_test, y_test),\n",
    "                                        verbose=1,\n",
    "                                        epochs=MAX_EPOCHS,\n",
    "                                        callbacks=[checkpoint_cb, early_stopping_cb], # , tensorboard_callback , reducelr_cb\n",
    "                                        batch_size = BATCH_SIZE)\n",
    "\n",
    "                                    model = tf.keras.models.load_model(f\"{models_path}/{model_name}.h5\")\n",
    "\n",
    "                                    # Calculate r-squared\n",
    "                                    test_predictions = model.predict(x_test, batch_size=BATCH_SIZE)\n",
    "                                    R2_score = r2_score(y_test, test_predictions)\n",
    "                                    plt.close(\"all\")\n",
    "                                    index = len(results)\n",
    "                                    fig = plot(history, model, x_test, y_train, y_test, features_string, BATCH_SIZE, index, R2_score, test_predictions)\n",
    "                                    plt.savefig(\n",
    "                                        f'{graphs_path}/{model_name}.png',\n",
    "                                        bbox_inches='tight',\n",
    "                                        dpi = 300\n",
    "                                    )\n",
    "\n",
    "                                    # plt.show(fig)\n",
    "\n",
    "                                    # Portfolio creation\n",
    "                                    x_train_dates = [i[0] for i in x_train.index]\n",
    "                                    x_train_stock = [i[1] for i in x_train.index]\n",
    "                                    train_predictions = model.predict(x_train, batch_size=BATCH_SIZE)\n",
    "                                    portfolio_data = np.array([x_train_dates,\n",
    "                                                               x_train_stock,\n",
    "                                                               x_train.iloc[:,-1],\n",
    "                                                               y_train.values,\n",
    "                                                               train_predictions[:,0]]\n",
    "                                                              )\n",
    "                                    portfolio_data = pd.DataFrame(portfolio_data.T,\n",
    "                                                                  columns = [\"date\", \"stock\", \"current price\", \"true 1 year\", \"predicted 1 year\"])\n",
    "\n",
    "                                    portfolio_data = portfolio_data.set_index(\"date\")\n",
    "                                    portfolio_data = portfolio_data.sort_values(by=[\"date\", \"predicted 1 year\"], ascending=[True, False])\n",
    "\n",
    "                                    rois_data = []\n",
    "                                    pbar2 = tqdm(np.linspace(0,1,NUMBER_OF_RATES+1)[1:], desc=\"Trying different rates\", total=NUMBER_OF_RATES)\n",
    "                                    for rate in pbar2:\n",
    "                                        rate = np.round(rate,3)\n",
    "                                        tups = []\n",
    "                                        for i in range(NUMBER_OF_RATES):\n",
    "                                            tups.append((rate, portfolio_data))\n",
    "                                        tups_tqdm = tqdm(tups, desc=f\"Getting rois for rate: {rate}\", total=len(tups))\n",
    "                                        rois = p.map(rois_func, tups_tqdm)\n",
    "                                        rois_data.append(np.vstack(rois))\n",
    "                                    pbar2.close()\n",
    "\n",
    "                                    rois_data = np.vstack(rois_data)\n",
    "                                    rois_data = pd.DataFrame(rois_data, columns=[\"rate\", \"true roi\", \"expected roi\", \"num unique stocks\"])\n",
    "                                    # Drop nan rows as those represent times when all stocks were expected to go down\n",
    "                                    rois_data = rois_data.dropna()\n",
    "\n",
    "                                    best_rate = rois_data[[\"true roi\", \"rate\"]].groupby([\"rate\"]).min().idxmax(axis=\"rows\")[0]\n",
    "                                    best_rate_min_roi = rois_data[[\"true roi\", \"rate\"]].groupby([\"rate\"]).min().max(axis=\"rows\")[0]\n",
    "\n",
    "                                    plt.figure(figsize=(13, 5), dpi=300)\n",
    "\n",
    "                                    # We are only going to invest when we expect a positive roi so only consider those.\n",
    "                                    positive_rois_data = rois_data.loc[rois_data[\"expected roi\"]>0,:]\n",
    "                                    plt.axhline(0, color=\"green\")\n",
    "                                    plt.axhline(-50, color=\"yellow\")\n",
    "                                    plt.axhline(-100, color=\"red\")\n",
    "                                    sns.boxplot(data=positive_rois_data, x=\"rate\", y=\"true roi\") # , showfliers = False\n",
    "                                    plt.title(f\"best rate (lowest risk)={best_rate} with worst roi of: {round(best_rate_min_roi,2)}\")\n",
    "                                    plt.savefig(f\"{portfolio_creation_training_path}/{model_name}.jpg\")\n",
    "\n",
    "\n",
    "                                    # Portfolio creation - interactive plot for testing data\n",
    "                                    features_dates = [i[0] for i in model_data.iloc[:,:-1].index]\n",
    "                                    features_stock = [i[1] for i in model_data.iloc[:,:-1].index]\n",
    "                                    features_predictions = model.predict(model_data.iloc[:,:-1], batch_size=BATCH_SIZE)\n",
    "                                    portfolio_data = np.array([features_dates,\n",
    "                                                               features_stock,\n",
    "                                                               model_data.iloc[:,:-1].iloc[:,-1],\n",
    "                                                               model_data.iloc[:,-1].values,\n",
    "                                                               features_predictions[:,0]]\n",
    "                                                              )\n",
    "                                    portfolio_data = pd.DataFrame(portfolio_data.T,\n",
    "                                                                  columns = [\"date\", \"stock\", \"current price\", \"true 1 year\", \"predicted 1 year\"])\n",
    "\n",
    "                                    portfolio_data = portfolio_data.sort_values(by=[\"date\", \"predicted 1 year\"], ascending=[True, False])\n",
    "                                    portfolio_data = portfolio_data.set_index(\"date\")\n",
    "                                    port_index = portfolio_data.index.unique()\n",
    "\n",
    "                                    port_index_tqdm = tqdm(enumerate(port_index), desc=\"Getting portfolio rois\", total=len(port_index))\n",
    "                                    rois = np.zeros((len(port_index),2))\n",
    "                                    extra_data = []\n",
    "                                    for i, date in port_index_tqdm:\n",
    "                                        if date in test_dates:\n",
    "                                            dataset_string = \"test\"\n",
    "                                        else:\n",
    "                                            dataset_string = \"train\"\n",
    "\n",
    "                                        # filter out relevant rows\n",
    "                                        current_df = portfolio_data.loc[date,:]\n",
    "\n",
    "                                        # Get array containing expected roi for each stock\n",
    "                                        expected_roi_per_stock = ((current_df[\"predicted 1 year\"].values - current_df[\"current price\"].values)/current_df[\"current price\"].values)*100\n",
    "                                        current_df[\"expected roi\"] = expected_roi_per_stock\n",
    "\n",
    "                                        current_df = current_df.sort_values([\"expected roi\"], ascending=False)\n",
    "                                        current_prices = current_df[\"current price\"]\n",
    "                                        current_stocks = current_df[\"stock\"]\n",
    "                                        expected_roi_per_stock = current_df[\"expected roi\"]\n",
    "\n",
    "                                        weights, num_stocks = portfolio_weights(best_rate, expected_roi_per_stock)\n",
    "\n",
    "                                        stocks_to_show = 10\n",
    "                                        weights_string = \" \".join(list([str(round(num, 2)) for num in list(weights)[:stocks_to_show]]))\n",
    "                                        stocks_string = \" \".join(list(current_stocks)[:stocks_to_show])\n",
    "                                        current_prices_string = \" \".join(list([str(round(num, 2)) for num in list(current_prices)[:stocks_to_show]]))\n",
    "                                        predicted_prices_string = \" \".join(list([str(round(num, 2)) for num in list(current_df[\"predicted 1 year\"])])[:stocks_to_show])\n",
    "                                        expected_roi_string = \" \".join(list([str(round(num, 2)) for num in list(expected_roi_per_stock)[:stocks_to_show]]))\n",
    "                                        true_price_string = \" \".join(list([str(round(num, 2)) for num in list(current_df[\"true 1 year\"])[:stocks_to_show]]))\n",
    "\n",
    "                                        current_portfolio_wealth = sum([weights[w] * decimal.Decimal(current_df[\"current price\"][w]) for w in range(len(weights))])\n",
    "                                        current_portfolio_wealth = float(current_portfolio_wealth)\n",
    "\n",
    "\n",
    "                                        future_true_wealth = sum([weights[w] * decimal.Decimal(current_df.loc[:, \"true 1 year\"][w]) for w in range(len(weights))])\n",
    "                                        future_true_wealth = float(future_true_wealth)\n",
    "\n",
    "\n",
    "                                        future_predicted_wealth = sum([weights[w] * decimal.Decimal(current_df.loc[:, \"predicted 1 year\"][w]) for w in range(len(weights))])\n",
    "                                        future_predicted_wealth = float(future_predicted_wealth)\n",
    "\n",
    "                                        if not current_portfolio_wealth == 0:\n",
    "                                            true_roi = ((future_true_wealth-current_portfolio_wealth)/current_portfolio_wealth)*100\n",
    "                                            expected_roi = ((future_predicted_wealth-current_portfolio_wealth)/current_portfolio_wealth)*100\n",
    "                                        else:\n",
    "                                            # The nans represent days when no stocks were expected to increase in price after 1 year\n",
    "                                            true_roi = np.nan\n",
    "                                            expected_roi = np.nan\n",
    "\n",
    "                                        rois[i, :] = np.array([true_roi, expected_roi])\n",
    "                                        # Append data\n",
    "                                        extra_data.append([stocks_string, current_prices_string, true_price_string, predicted_prices_string, expected_roi_string, num_stocks, weights_string, dataset_string])\n",
    "\n",
    "                                    rois_data = pd.DataFrame(rois, columns = [\"true roi\", \"expected roi\"], index=port_index)\n",
    "                                    extra_data = pd.DataFrame(extra_data,\n",
    "                                                              columns = [\"stocks\", \"current stock prices\", \"1Y true prices\", \"1Y predicted prices\", \"stocks expected roi\", \"num unique stocks\", \"weights\", \"dataset\"],\n",
    "                                                              index=port_index)\n",
    "                                    extra_data[\"date\"] = [str(date.date()) for date in extra_data.index]\n",
    "\n",
    "                                    rois_data = pd.concat([rois_data, extra_data], axis=\"columns\", ignore_index=False)\n",
    "                                    # Drop nan rows as those represent times when all stocks were expected to go down\n",
    "                                    rois_data = rois_data.dropna()\n",
    "\n",
    "                                    print(\"Creating interactive scatterplot\")\n",
    "                                    r_squared_train = r2_score(rois_data.loc[rois_data.dataset == \"train\", \"true roi\"], rois_data.loc[rois_data.dataset == \"train\", \"expected roi\"])\n",
    "                                    r_squared_test = r2_score(rois_data.loc[rois_data.dataset == \"test\", \"true roi\"], rois_data.loc[rois_data.dataset == \"test\", \"expected roi\"])\n",
    "                                    fig = px.scatter(rois_data,\n",
    "                                                     x=\"true roi\", y=\"expected roi\",\n",
    "                                                     title = f\"Train r2 = {round(r_squared_train,3)}, Test r2 = {round(r_squared_test,3)}\",\n",
    "                                                     hover_data=extra_data.columns,\n",
    "                                                     marginal_x=\"box\", marginal_y=\"box\",\n",
    "                                                     color = \"dataset\"\n",
    "                                                     )\n",
    "                                    # Add a line trace with y=x to the figure\n",
    "                                    fig.add_trace(go.Scatter(x=[rois_data[\"true roi\"].min(), rois_data[\"true roi\"].max()],\n",
    "                                                             y=[rois_data[\"true roi\"].min(), rois_data[\"true roi\"].max()],\n",
    "                                                             mode='lines',\n",
    "                                                             name='y=x'))\n",
    "\n",
    "                                    print(\"Saving interactive scatterplot\")\n",
    "                                    fig.write_html(f\"{portfolio_creation_testing_path}/{model_name}.html\")\n",
    "                                    print(\"Saved interactive scatterplot\")\n",
    "\n",
    "\n",
    "                                    # Add result to `results` dataframe and to \"model results.csv\"\n",
    "                                    total_neurons = layer * neuron\n",
    "                                    result_row = [features_string, NUMBER_OF_LINES, NUMBER_OF_YEARS_TO_CONSIDER, layer, neuron, total_neurons, BATCH_SIZE, technique, R2_score]\n",
    "                                    results = results.append({\"features\":features_string,\n",
    "                                                              \"lines\":NUMBER_OF_LINES,\n",
    "                                                              \"years\":NUMBER_OF_YEARS_TO_CONSIDER,\n",
    "                                                              \"layers\":layer,\n",
    "                                                              \"neurons per layer\":neuron,\n",
    "                                                              \"total neurons\":total_neurons,\n",
    "                                                              \"batch size\":BATCH_SIZE,\n",
    "                                                              \"technique\":technique,\n",
    "                                                              \"val_r_square\":R2_score}, ignore_index=True)\n",
    "\n",
    "\n",
    "                                    with open(f\"{root_folder}/{overview_folder}/model results.csv\", \"a\") as f:\n",
    "                                        writer = csv.writer(f)\n",
    "                                        writer.writerow(result_row)\n",
    "                                else:\n",
    "                                    print(f\"\\n\"\n",
    "                                          f\"ALREADY COMPLETED model:\\n\"\n",
    "                                          f\"Data: {data_name}\\n\"\n",
    "                                          f\"Layers: {layer}\\n\"\n",
    "                                          f\"Neurons: {neuron}\\n\"\n",
    "                                          f\"Batch size = {BATCH_SIZE}\\n\"\n",
    "                                          f\"Technique = {technique}\\n\")\n",
    "                                pbar.update(1)\n",
    "\n",
    "            # Now create the interactive plots\n",
    "            for col in results.columns[:-1]:\n",
    "                fig = px.scatter(results, x=col, y=\"val_r_square\",\n",
    "                                 color = \"features\",\n",
    "                                 size = list(results[\"total neurons\"]),\n",
    "                                 hover_data=results.columns[:-1],\n",
    "                                 title = f\"R2 score against {col}\"\n",
    "                                 )\n",
    "                fig.update_yaxes(range = [0, 1.05])\n",
    "                fig.write_html(f\"{root_folder}/{overview_folder}/R2 score against {col}.html\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Condition not met: NUMBER_OF_LINES <= NUMBER_OF_YEARS_TO_CONSIDER*365\")\n",
    "    else:\n",
    "        pbar.update(len(feature_combinations)*len(layers)*len(neurons)*len(batch_sizes)*len(techniques))\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
