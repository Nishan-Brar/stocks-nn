{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import decimal\n",
    "\n",
    "disable_progress_bars = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If cannot connect to local hosot, run both of the following commands in cmd using the code block below:\n",
    "* `taskkill /im tensorboard.exe /f`\n",
    "* `del /q %TMP%\\.tensorboard-info\\*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.system('cmd /k \"taskkill /im tensorboard.exe /f\"')\n",
    "# os.system('cmd /k \"del /q %TMP%\\.tensorboard-info\\*\"')\n",
    "#\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir results --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:38.355211Z",
     "start_time": "2023-01-05T12:27:38.341552Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip uninstall -y tensorflow\n",
    "# ! pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:47.124593Z",
     "start_time": "2023-01-05T12:27:38.357356Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from numba import jit\n",
    "from multiprocessing import Pool\n",
    "from outer_function_daily import outer_function\n",
    "from rois import rois_func, portfolio_weights\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import fredapi as fa\n",
    "fred = fa.Fred(api_key=\"5b3325018adf679644bc330c41598801\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import csv\n",
    "import random\n",
    "import datetime\n",
    "import networkx as nx\n",
    "\n",
    "import yfinance as yf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:47.153857Z",
     "start_time": "2023-01-05T12:27:47.124593Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Check if it detects GPU\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:52.288577Z",
     "start_time": "2023-01-05T12:27:47.211965Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "stocks_df = pd.read_csv(\"data/price.csv\")\n",
    "\n",
    "# Set date-time index\n",
    "stocks_df['Date'] = pd.to_datetime(stocks_df[\"Date\"], format='%Y/%m/%d')\n",
    "stocks_df = stocks_df.set_index(stocks_df['Date'])\n",
    "stocks_df = stocks_df.drop(columns = [\"Date\"])\n",
    "\n",
    "# Drop all columns which only contain nan values\n",
    "boolean = []\n",
    "for stock in stocks_df.columns:\n",
    "    boolean.append(not stocks_df[stock].isnull().all())\n",
    "stocks_df = stocks_df.iloc[:, boolean]\n",
    "\n",
    "describe = stocks_df.describe()\n",
    "stocks_df = stocks_df[describe.loc[\"min\",:][describe.loc[\"min\",:]>=0].index]\n",
    "\n",
    "stocks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check amount of data per date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.394906Z",
     "start_time": "2023-01-05T12:27:52.288577Z"
    }
   },
   "outputs": [],
   "source": [
    "notnan = []\n",
    "for i in range(len(stocks_df)):\n",
    "    notnan.append(stocks_df.shape[1] - stocks_df.iloc[i,:].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.612371Z",
     "start_time": "2023-01-05T12:27:54.394906Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(stocks_df.index, notnan)\n",
    "plt.title(\"Not-nan stocks per date\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"Not-nan stocks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.628143Z",
     "start_time": "2023-01-05T12:27:54.615358Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "first_date = stocks_df.index[0]\n",
    "last_date = stocks_df.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.865770Z",
     "start_time": "2023-01-05T12:27:54.642845Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cat_variables = pd.read_csv(\"data/categorical.csv\")\n",
    "cat_variables = cat_variables.set_index(\"Unnamed: 0\")\n",
    "cat_variables = cat_variables.rename_axis(None, axis = 0)\n",
    "cat_variables = cat_variables.dropna(axis=\"columns\")\n",
    "\n",
    "# Drop stocks such that the remaining stocks are those in both cat_variables and stocks_df\n",
    "intersection = list(set(stocks_df.columns) & set(cat_variables.columns))\n",
    "cat_variables = cat_variables[intersection]\n",
    "stocks_df = stocks_df[intersection]\n",
    "\n",
    "cat_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.881881Z",
     "start_time": "2023-01-05T12:27:54.868869Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_variables_np = cat_variables.to_numpy()\n",
    "cat_variables_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:54.897130Z",
     "start_time": "2023-01-05T12:27:54.883312Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Getting categories for each categorical variable\n",
    "################## THIS MIGHT GET AN ERROR AS YOU ADD MORE CATEGORICAL VARIABLES SINCE THE COLUMNS BEING ADDED ARE DIFFERENT LENGTHS BUT\n",
    "################## IDK IF IT WILL AUTOMATICALLY REPLACE WITH NAN VALUES !!!!!!!!!!!!\n",
    "cats_df = pd.DataFrame()\n",
    "for cat in cat_variables.index:\n",
    "    uniques = list(cat_variables.loc[cat, :].unique())\n",
    "    categories = np.sort(uniques)\n",
    "    cats_df[cat] = categories\n",
    "\n",
    "cats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.040653Z",
     "start_time": "2023-01-05T12:27:54.970478Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features_daily = pd.read_csv(\"data/features.csv\")\n",
    "features_daily['Date'] = pd.to_datetime(features_daily[\"Date\"], format='%Y/%m/%d')\n",
    "features_daily = features_daily.set_index(\"Date\")\n",
    "features_daily = features_daily.dropna()\n",
    "\n",
    "# Make names smaller to use in file names\n",
    "print(features_daily.columns)\n",
    "features_daily.columns = ['BCI', 'CCI', 'CLI', '3m %', 'Broad $', 'Construction', 'Consumer prices', 'Manu $',\n",
    "       'Indu production', 'Long %', 'MCI', 'Narrow $', 'Overnight %', 'Cars', 'producer $', 'Retail volume',\n",
    "       'Total employment', 'Total manu', 'Yield curve']\n",
    "\n",
    "features_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.056594Z",
     "start_time": "2023-01-05T12:27:55.043690Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features_daily_np = features_daily.to_numpy()\n",
    "features_daily_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.072276Z",
     "start_time": "2023-01-05T12:27:55.058689Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_feature_index(feature):\n",
    "    \"\"\"\n",
    "    Returns the index in the numpy feature array using the features name.\n",
    "    \"\"\"\n",
    "    return list(features_daily.columns).index(feature)\n",
    "\n",
    "def get_cat_index(cat):\n",
    "    \"\"\"\n",
    "    Returns the index in the numpy feature array using the features name.\n",
    "    \"\"\"\n",
    "    return list(cat_variables.index).index(cat)\n",
    "\n",
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.098213Z",
     "start_time": "2023-01-05T12:27:55.076423Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_EPOCHS = 200\n",
    "PATIENCE = 15\n",
    "NUMBER_OF_RATES = 3\n",
    "\n",
    "# Neural network hyperparameters to try\n",
    "layers = [2, 3, 4]\n",
    "neurons = [32, 64, 128]\n",
    "\n",
    "# Techniques\n",
    "techniques = [\"base\", \"dropout layers\", \"batch regularisation\", \"dropout & batch\"]\n",
    "\n",
    "# Hyperparameters\n",
    "years = [3, 4, 5]\n",
    "lines = [16, 32,64]\n",
    "batch_sizes = [2048]\n",
    "# The categorical and numeric features included in the data are the ones at the SAME INDEX !!!\n",
    "cats_subsets = [[\"sectors\"]]\n",
    "features_subsets = [[\"Manu $\", \"Narrow $\", \"Cars\", \"Indu production\"]]\n",
    "\n",
    "\n",
    "if len(cats_subsets) != len(features_subsets):\n",
    "    raise ValueError(\"The length of `cats_subsets` & `features_subsets` must be the same since the features included in the model \"\n",
    "                     \"are those at the same indexes of these lists.\")\n",
    "all_cats = set()\n",
    "for subset in cats_subsets:\n",
    "    for var in subset:\n",
    "        all_cats.add(var)\n",
    "all_features = set()\n",
    "for subset in features_subsets:\n",
    "    for var in subset:\n",
    "        all_features.add(var)\n",
    "\n",
    "# Sort for consistency\n",
    "all_cats = sorted(list(all_cats))\n",
    "all_features = sorted(list(all_features))\n",
    "\n",
    "all_cats_indexes = []\n",
    "for var in all_cats:\n",
    "    all_cats_indexes.append(get_cat_index(var))\n",
    "all_features_indexes = []\n",
    "for var in all_features:\n",
    "    all_features_indexes.append(get_feature_index(var))\n",
    "\n",
    "\n",
    "##### FOR `cats_subsets`\n",
    "# Sort the names so that the name of the model files and graphs will always be the same regardless of the order of the names in the subset\n",
    "# which allows the program to skip creating the model as it has already been created.\n",
    "for i in range(len(cats_subsets)):\n",
    "    cats_subsets[i].sort()\n",
    "\n",
    "cats_indexes = []\n",
    "for subset_names in cats_subsets:\n",
    "    subset_indexes = []\n",
    "    for feature in subset_names:\n",
    "        subset_indexes.append(get_cat_index(feature))\n",
    "    cats_indexes.append(subset_indexes)\n",
    "\n",
    "\n",
    "##### FOR `features_subsets`\n",
    "# Sort the names so that the name of the model files and graphs will always be the same regardless of the order of the names in the subset\n",
    "# which allows the program to skip creating the model as it has already been created.\n",
    "for i in range(len(features_subsets)):\n",
    "    features_subsets[i].sort()\n",
    "\n",
    "features_subsets_indexes = []\n",
    "for subset_names in features_subsets:\n",
    "    subset_indexes = []\n",
    "    for feature in subset_names:\n",
    "        subset_indexes.append(get_feature_index(feature))\n",
    "    features_subsets_indexes.append(subset_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural network on created data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.113564Z",
     "start_time": "2023-01-05T12:27:55.100287Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stocks_np = stocks_df.to_numpy()\n",
    "stocks_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Very good explanation of [multiprocessing mapping methods](https://stackoverflow.com/questions/26520781/multiprocessing-pool-whats-the-difference-between-map-async-and-imap).\n",
    "* Good explanation of affect of [optimal chunksize](https://stackoverflow.com/questions/34988692/python-3-multiprocessing-optimal-chunk-size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I THINK THERE MAY STILL BE ROOM FOR IMPROVEMENT FOR IN MULTIPROCESSING BY VARYING CHUNKSIZE IN `imap` METHOD.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.129251Z",
     "start_time": "2023-01-05T12:27:55.115873Z"
    }
   },
   "outputs": [],
   "source": [
    "def graph_already_completed(graph_path, layer, neuron):\n",
    "    completed_graphs = os.listdir(graph_path)\n",
    "    for graph_name in completed_graphs:\n",
    "        if f\"{layer}L-{neuron}N\" in graph_name:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.160287Z",
     "start_time": "2023-01-05T12:27:55.130566Z"
    }
   },
   "outputs": [],
   "source": [
    "root_folder = \"results (daily)\"\n",
    "overview_folder = \"overview\"\n",
    "details_folder = \"details\"\n",
    "\n",
    "# Create/load file to store results of each model\n",
    "try:\n",
    "    os.makedirs(f\"{root_folder}/{overview_folder}\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "\n",
    "model_results_path = f\"{root_folder}/{overview_folder}/model results.csv\"\n",
    "try:\n",
    "    results = pd.read_csv(model_results_path)\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist then we create the file\n",
    "    with open(model_results_path, \"w\") as f:\n",
    "        f.write(\",\".join([\"features\",\"lines\", \"years\", \"layers\", \"neurons per layer\", \"total neurons\", \"batch size\", \"technique\", \"val_r_square\"]) + \"\\n\")\n",
    "    results = pd.read_csv(model_results_path)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.175694Z",
     "start_time": "2023-01-05T12:27:55.161383Z"
    }
   },
   "outputs": [],
   "source": [
    "# This also ensures that all the hyperparameters sets where no features (other than stock price) are considered always run first as they\n",
    "# act as a base to compare to.\n",
    "base_hyperparameter_combinations = []\n",
    "for NUMBER_OF_LINES in lines:\n",
    "    for NUMBER_OF_YEARS_TO_CONSIDER in years:\n",
    "        tup = (NUMBER_OF_LINES, NUMBER_OF_YEARS_TO_CONSIDER)\n",
    "        base_hyperparameter_combinations.append(tup)\n",
    "\n",
    "base_hyperparameter_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.192106Z",
     "start_time": "2023-01-05T12:27:55.177697Z"
    }
   },
   "outputs": [],
   "source": [
    "# This also ensures that all the hyperparameters sets where no features (other than stock price) are considered always run first as they\n",
    "# act as a base to compare to.\n",
    "feature_combinations = []\n",
    "for (subset_i, features_subset), (subset_j, cats_index) in zip(enumerate(features_subsets_indexes), enumerate(cats_indexes)):\n",
    "    tup = ((subset_i, features_subset), (subset_j, cats_index))\n",
    "    feature_combinations.append(tup)\n",
    "\n",
    "feature_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping rows in results for models to redo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.207868Z",
     "start_time": "2023-01-05T12:27:55.194371Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_model_by_features(features_string, lines, years, layers, neurons_per_layer, results):\n",
    "    index = results.loc[(results.features==features_string) &\n",
    "                        (results.lines==lines) &\n",
    "                        (results.years==years) &\n",
    "                        (results.layers==layers) &\n",
    "                        (results[\"neurons per layer\"]==neurons_per_layer),:].index[0]\n",
    "    results = results.drop(index, axis=0)\n",
    "    return results\n",
    "\n",
    "def remove_model_by_index(indexes, results):\n",
    "    for i in indexes:\n",
    "        results = results.drop(i, axis=0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove by features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.223087Z",
     "start_time": "2023-01-05T12:27:55.209943Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Table of models to redo (By features)\n",
    "#\n",
    "# table=[[]]\n",
    "# for m in table:\n",
    "#     features_string, lines, years, layers, neurons_per_layer = m\n",
    "#     results = remove_model_by_features(features_string, lines, years, layers, neurons_per_layer, results)\n",
    "# results.to_csv(\"model results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.238548Z",
     "start_time": "2023-01-05T12:27:55.225333Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # List of indexes to remove to redo the models\n",
    "#\n",
    "# list_of_index = []\n",
    "# results = remove_model_by_index(list_of_index, results)\n",
    "# results.to_csv(\"model results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.253812Z",
     "start_time": "2023-01-05T12:27:55.240604Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot(history, model, x_test, y_train, y_test, features_string, batch_size, i, R2, test_predictions):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13, 5), dpi=300)\n",
    "\n",
    "    # PLot actual vs prediction scatter plot\n",
    "    test_predictions = test_predictions.flatten()\n",
    "    ax[0].scatter(y_test, test_predictions, alpha=0.01)\n",
    "    ax[0].set_xlabel('True Values')\n",
    "    ax[0].set_ylabel('Predictions')\n",
    "\n",
    "    # Plot y=x line\n",
    "    # ax[0].axline((0, 0), slope=1, c='green', linewidth=2)\n",
    "    points = [0, max(max(y_test), max(test_predictions))]\n",
    "    ax[0].plot(points, points, color=\"green\", linewidth = 3)\n",
    "\n",
    "    # Plot the average line\n",
    "    # ax[0].axline((0, np.mean(y_train)), slope=0, c='red', linewidth=2, label='mean')\n",
    "    x = [min(y_test), max(y_test)]\n",
    "    y = [np.mean(y_train), np.mean(y_train)]\n",
    "    ax[0].plot(x, y, color=\"red\", linewidth = 3)\n",
    "    ax[0].set_title(\"True values against predictions\")\n",
    "\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].set_xscale('log')\n",
    "\n",
    "    # Plot training and testing r-squared\n",
    "    ax[1].plot(history.history['r_square'], label='r_square')\n",
    "    ax[1].plot(history.history['val_r_square'], label='val_r_square')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Error')\n",
    "    ax[1].set_title('R-Squared')\n",
    "    ax[1].set_ylim([0, 1])\n",
    "    ax[1].set_yticks(np.arange(0, 1.05, 0.05))\n",
    "    # xticks = np.arange(len(history.history['val_r_square'])) + 1\n",
    "    # ax[1].set_xticks(xticks.astype(int))\n",
    "\n",
    "    fig.suptitle(f\"Test R2={round(R2,3)}, BS={batch_size}, result index={i}, {features_string}\", fontsize=\"small\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.269156Z",
     "start_time": "2023-01-05T12:27:55.255812Z"
    }
   },
   "outputs": [],
   "source": [
    "def models_remaining(layers, neurons, batch_sizes, techniques,feature_combinations, models_to_redo_by_index):\n",
    "    for layer in layers:\n",
    "        for neuron in neurons:\n",
    "            for BATCH_SIZE in batch_sizes:\n",
    "                for technique in techniques:\n",
    "                    for (subset_i, features_subset), (subset_j, cats_index) in feature_combinations:\n",
    "                        features_string = f\"{cats_subsets[subset_j] + features_subsets[subset_i]}\".replace(\"'\",\"\").replace(\",\",\"\")\n",
    "                        row = [features_string, NUMBER_OF_LINES, NUMBER_OF_YEARS_TO_CONSIDER, layer, neuron, layer*neuron, BATCH_SIZE, technique]\n",
    "\n",
    "                        # The following determines if the current `row` index was designated to be trained further. The try-except clause takes into account the case\n",
    "                        # where the `row` doesn't exist in `results` df.\n",
    "                        try:\n",
    "                            idx = (results[results.columns[:-1]] == row).index[0]\n",
    "                        except IndexError:\n",
    "                            boolean = False\n",
    "                        else:\n",
    "                            boolean = idx in models_to_redo_by_index\n",
    "                        # If the model has been trained or needs to be trained more, we return True\n",
    "                        if not (results[results.columns[:-1]] == row).all(1).any() or boolean:\n",
    "                            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T12:27:55.285082Z",
     "start_time": "2023-01-05T12:27:55.269367Z"
    }
   },
   "outputs": [],
   "source": [
    "models_to_redo_by_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-05T13:00:29.612666Z",
     "start_time": "2023-01-05T12:27:55.287218Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = Pool(os.cpu_count())\n",
    "pbar = tqdm(total=len(base_hyperparameter_combinations)*len(feature_combinations)*len(layers)*len(neurons)*len(batch_sizes)*len(techniques), desc = \"Completed models\")\n",
    "for NUMBER_OF_LINES, NUMBER_OF_YEARS_TO_CONSIDER in base_hyperparameter_combinations:\n",
    "    if models_remaining(layers, neurons, batch_sizes, techniques,feature_combinations, models_to_redo_by_index):\n",
    "        if NUMBER_OF_LINES <= NUMBER_OF_YEARS_TO_CONSIDER*365:\n",
    "            first_index = NUMBER_OF_YEARS_TO_CONSIDER*365+1 # `+1` needed since slicing is end-exclusive\n",
    "            range_iterations = range(first_index, len(stocks_np) - 365)\n",
    "\n",
    "            iterations = tqdm(range_iterations,\n",
    "                              desc=f\"Working on Dataset: {NUMBER_OF_LINES} lines, {NUMBER_OF_YEARS_TO_CONSIDER} years\",\n",
    "                              disable=False)\n",
    "\n",
    "            result_list = p.map(partial(outer_function,\n",
    "                                        stocks_np = stocks_np,\n",
    "                                        NUMBER_OF_YEARS_TO_CONSIDER = NUMBER_OF_YEARS_TO_CONSIDER,\n",
    "                                        NUMBER_OF_LINES = NUMBER_OF_LINES,\n",
    "                                        features_subset_indexes = all_features_indexes,\n",
    "                                        features_np = features_daily_np,\n",
    "                                        cats_np = cat_variables_np,\n",
    "                                        cats_index = all_cats_indexes,\n",
    "                                        cats_df = cats_df,\n",
    "                                        dates = np.array(stocks_df.index),\n",
    "                                        stocks = np.array(stocks_df.columns)\n",
    "                                        ),\n",
    "                                iterations)\n",
    "\n",
    "            data = np.vstack([result[0] for result in result_list])\n",
    "            # data = data.astype(np.float64) ## I don't think this is needed\n",
    "            print(data)\n",
    "            data_dates = np.hstack([result[1] for result in result_list])\n",
    "            data_stocks = np.hstack([result[2] for result in result_list])\n",
    "\n",
    "            cats_dict = dict()\n",
    "            for k in all_cats_indexes:\n",
    "                cats_dict[k] = np.hstack([result_list[i][3][k] for i in range(len(result_list))])\n",
    "\n",
    "            features_dict = dict()\n",
    "            for k in all_features_indexes:\n",
    "                features_dict[k] = np.vstack([result_list[i][4][k] for i in range(len(result_list))])\n",
    "\n",
    "\n",
    "            # Creating neural networks\n",
    "            for layer in layers:\n",
    "                for neuron in neurons:\n",
    "                    for BATCH_SIZE in batch_sizes:\n",
    "                        for technique in techniques:\n",
    "                            for (subset_i, features_subset), (subset_j, cats_index) in feature_combinations:\n",
    "                                data_name = f\"{NUMBER_OF_LINES}Lines-{NUMBER_OF_YEARS_TO_CONSIDER}Years-{features_subsets[subset_i]}Features-{cats_subsets[subset_j]}Categorical\".replace(\"'\", \"\")\n",
    "                                features_string = f\"{cats_subsets[subset_j] + features_subsets[subset_i]}\".replace(\"'\",\"\").replace(\",\",\"\")\n",
    "                                row = [features_string, NUMBER_OF_LINES, NUMBER_OF_YEARS_TO_CONSIDER, layer, neuron, layer*neuron, BATCH_SIZE, technique]\n",
    "                                # The following determines if the current `row` index was designated to be trained further. The try-except clause takes into account the case\n",
    "                                # where the `row` doesn't exist in `results` df.\n",
    "                                try:\n",
    "                                    idx = (results[results.columns[:-1]] == row).index[0]\n",
    "                                except IndexError:\n",
    "                                    boolean = False\n",
    "                                else:\n",
    "                                    boolean = idx in models_to_redo_by_index\n",
    "                                    # If the model has been trained or needs to be trained more, we return True\n",
    "                                if not (results[results.columns[:-1]] == row).all(1).any() or boolean:\n",
    "\n",
    "                                    categorical_dummy_dfs_list = []\n",
    "                                    for c in cats_index:\n",
    "                                        categorical_dummy_dfs_list.append(pd.get_dummies(cats_dict[c]).iloc[:,:-1])\n",
    "\n",
    "\n",
    "                                    # The `data` list must go last since the rest of the script expects the dependent variable to be the last column !!\n",
    "                                    model_data = np.hstack(categorical_dummy_dfs_list + [features_dict[f] for f in features_subset] + [data])\n",
    "\n",
    "\n",
    "                                    # Create data for model\n",
    "                                    tuples = list(zip(data_dates, data_stocks))\n",
    "                                    index = pd.MultiIndex.from_tuples(tuples, names=[\"dates\", \"stocks\"])\n",
    "                                    model_data = pd.DataFrame(model_data, index=index)\n",
    "\n",
    "#############################################################################################################################################\n",
    "                                    model_data_train = pd.concat([model_data.loc[:\"2006-5-1\",:], model_data.loc[\"2014-5-1\":,:]], axis=\"rows\").sample(frac=1,\n",
    "                                                                                                                                                     random_state=1)\n",
    "                                    model_data_test = model_data.loc[\"2006-5-1\":\"2014-5-1\", :]\n",
    "#############################################################################################################################################\n",
    "                                    x_train = model_data_train.iloc[:,:-1]\n",
    "                                    y_train = model_data_train.iloc[:,-1]\n",
    "\n",
    "                                    x_test = model_data_test.iloc[:,:-1]\n",
    "                                    y_test = model_data_test.iloc[:,-1]\n",
    "#############################################################################################################################################\n",
    "                                    test_dates = model_data_test.index\n",
    "                                    \n",
    "                                    # Create folders\n",
    "                                    graphs_path = f'{root_folder}/{details_folder}/{technique}/models training & testing/{data_name}'\n",
    "                                    try:\n",
    "                                        os.makedirs(graphs_path)\n",
    "                                    except FileExistsError:\n",
    "                                        pass\n",
    "\n",
    "                                    models_path = f'{root_folder}/{details_folder}/{technique}/models/{data_name}'\n",
    "                                    try:\n",
    "                                        os.makedirs(models_path)\n",
    "                                    except FileExistsError:\n",
    "                                        pass\n",
    "\n",
    "                                    portfolio_creation_testing_path = f\"{root_folder}/{details_folder}/{technique}/investment results/testing/{data_name}\"\n",
    "                                    try:\n",
    "                                        os.makedirs(portfolio_creation_testing_path)\n",
    "                                    except FileExistsError:\n",
    "                                        pass\n",
    "\n",
    "                                    portfolio_creation_training_path = f\"{root_folder}/{details_folder}/{technique}/investment results/training/{data_name}\"\n",
    "                                    try:\n",
    "                                        os.makedirs(portfolio_creation_training_path)\n",
    "                                    except FileExistsError:\n",
    "                                        pass\n",
    "\n",
    "                                    model_name = f\"{BATCH_SIZE}BS-{layer}Layers-{neuron}Neurons\"\n",
    "\n",
    "                                    print(f\"\\n\"\n",
    "                                          f\"Working on model:\\n\"\n",
    "                                          f\"Data: {data_name}\\n\"\n",
    "                                          f\"Layers: {layer}\\n\"\n",
    "                                          f\"Neurons: {neuron}\\n\"\n",
    "                                          f\"Batch size = {BATCH_SIZE}\\n\"\n",
    "                                          f\"Technique = {technique}\\n\")\n",
    "\n",
    "                                    # Normalize the data\n",
    "                                    normalizer = preprocessing.Normalization(axis=-1)\n",
    "\n",
    "                                    # The following determines if the current `row` index was designated to be trained further. The try-except clause takes into account the case\n",
    "                                    # where the `row` doesn't exist in `results` df.\n",
    "                                    try:\n",
    "                                        idx = (results[results.columns[:-1]] == row).index[0]\n",
    "                                    except IndexError:\n",
    "                                        boolean = False\n",
    "                                    else:\n",
    "                                        boolean = idx in models_to_redo_by_index\n",
    "                                        # If the model has been trained or needs to be trained more, we return True\n",
    "                                    if not boolean:\n",
    "                                        model = Sequential()\n",
    "                                        model.add(normalizer)\n",
    "                                        if technique == \"dropout & batch\":\n",
    "                                            for _ in range(layer):\n",
    "                                                model.add(Dense(neuron))\n",
    "                                                model.add(BatchNormalization(axis=1))\n",
    "                                                model.add(Activation(\"relu\"))\n",
    "                                                model.add(Dropout(0.5))\n",
    "                                        else:\n",
    "                                            for _ in range(layer):\n",
    "                                                model.add(Dense(neuron))\n",
    "                                                if technique == \"batch regularisation\":\n",
    "                                                    model.add(BatchNormalization(axis=1))\n",
    "\n",
    "                                                model.add(Activation(\"relu\"))\n",
    "\n",
    "                                                if technique == \"dropout layers\":\n",
    "                                                    model.add(Dropout(0.5))\n",
    "                                        model.add(Dense(1))\n",
    "                                    else:\n",
    "                                        model = tf.keras.models.load_model(f\"{models_path}/{model_name}.h5\")\n",
    "\n",
    "                                    model.compile(loss='mean_absolute_error',\n",
    "                                                  optimizer= Adam(learning_rate=0.00001),\n",
    "                                                  metrics=[RSquare()])\n",
    "\n",
    "                                    checkpoint_cb = ModelCheckpoint(f\"{models_path}/{model_name}.h5\",\n",
    "                                                                    monitor='val_r_square',\n",
    "                                                                    save_best_only=True,\n",
    "                                                                    verbose=0,\n",
    "                                                                    save_weights_only=False,\n",
    "                                                                    mode = \"max\")\n",
    "                                    early_stopping_cb = EarlyStopping(monitor = \"val_r_square\",\n",
    "                                                                      patience = PATIENCE, # No. epochs with no improvement after which training is stopped.\n",
    "                                                                      restore_best_weights=True,\n",
    "                                                                      verbose = 0,\n",
    "                                                                      min_delta = 0.01, # Increase of 0.01 per `PATIENCE` iterations required at least\n",
    "                                                                      mode = \"max\")\n",
    "\n",
    "                                    \n",
    "                                    # tensorboard_callback = tf.keras.callbacks.TensorBoard(f\"{root_folder}/{details_folder}/{technique}/logs/{data_name}/{model_name}\")\n",
    "\n",
    "                                    # reducelr_cb = ReduceLROnPlateau(monitor='val_r_square',\n",
    "                                    #                                 factor=0.5,\n",
    "                                    #                                 patience=REDUCE_LR_PATIENCE,\n",
    "                                    #                                 verbose = 0,\n",
    "                                    #                                 mode = \"max\")\n",
    "\n",
    "                                    history = model.fit(\n",
    "                                        x_train, y_train,\n",
    "                                        validation_data = (x_test, y_test),\n",
    "                                        verbose=1,\n",
    "                                        epochs=MAX_EPOCHS,\n",
    "                                        callbacks=[checkpoint_cb, early_stopping_cb], # , tensorboard_callback , reducelr_cb\n",
    "                                        batch_size = BATCH_SIZE)\n",
    "\n",
    "                                    model = tf.keras.models.load_model(f\"{models_path}/{model_name}.h5\")\n",
    "\n",
    "                                    # Calculate r-squared\n",
    "                                    test_predictions = model.predict(x_test, batch_size=BATCH_SIZE)\n",
    "                                    R2_score = r2_score(y_test, test_predictions)\n",
    "                                    plt.close(\"all\")\n",
    "                                    index = len(results)\n",
    "                                    fig = plot(history, model, x_test, y_train, y_test, features_string, BATCH_SIZE, index, R2_score, test_predictions)\n",
    "                                    plt.savefig(\n",
    "                                        f'{graphs_path}/{model_name}.png',\n",
    "                                        bbox_inches='tight',\n",
    "                                        dpi = 300\n",
    "                                    )\n",
    "\n",
    "                                    # plt.show(fig)\n",
    "\n",
    "                                    # Portfolio creation\n",
    "                                    x_train_dates = [i[0] for i in x_train.index]\n",
    "                                    x_train_stock = [i[1] for i in x_train.index]\n",
    "                                    train_predictions = model.predict(x_train, batch_size=BATCH_SIZE)\n",
    "                                    portfolio_data = np.array([x_train_dates,\n",
    "                                                               x_train_stock,\n",
    "                                                               x_train.iloc[:,-1],\n",
    "                                                               y_train.values,\n",
    "                                                               train_predictions[:,0]]\n",
    "                                                              )\n",
    "                                    portfolio_data = pd.DataFrame(portfolio_data.T,\n",
    "                                                                  columns = [\"date\", \"stock\", \"current price\", \"true 1 year\", \"predicted 1 year\"])\n",
    "\n",
    "                                    portfolio_data = portfolio_data.set_index(\"date\")\n",
    "                                    portfolio_data = portfolio_data.sort_values(by=[\"date\", \"predicted 1 year\"], ascending=[True, False])\n",
    "\n",
    "                                    rois_data = []\n",
    "                                    pbar2 = tqdm(np.linspace(0,1,NUMBER_OF_RATES+1)[1:], desc=\"Trying different rates\", total=NUMBER_OF_RATES)\n",
    "                                    for rate in pbar2:\n",
    "                                        rate = np.round(rate,3)\n",
    "                                        tups = []\n",
    "                                        for i in range(NUMBER_OF_RATES):\n",
    "                                            tups.append((rate, portfolio_data))\n",
    "                                        tups_tqdm = tqdm(tups, desc=f\"Getting rois for rate: {rate}\", total=len(tups))\n",
    "                                        rois = p.map(rois_func, tups_tqdm)\n",
    "                                        rois_data.append(np.vstack(rois))\n",
    "                                    pbar2.close()\n",
    "\n",
    "                                    rois_data = np.vstack(rois_data)\n",
    "                                    rois_data = pd.DataFrame(rois_data, columns=[\"rate\", \"true roi\", \"expected roi\", \"num unique stocks\"])\n",
    "                                    # Drop nan rows as those represent times when all stocks were expected to go down\n",
    "                                    rois_data = rois_data.dropna()\n",
    "\n",
    "                                    best_rate = rois_data[[\"true roi\", \"rate\"]].groupby([\"rate\"]).min().idxmax(axis=\"rows\")[0]\n",
    "                                    best_rate_min_roi = rois_data[[\"true roi\", \"rate\"]].groupby([\"rate\"]).min().max(axis=\"rows\")[0]\n",
    "\n",
    "                                    plt.figure(figsize=(13, 5), dpi=300)\n",
    "\n",
    "                                    # We are only going to invest when we expect a positive roi so only consider those.\n",
    "                                    positive_rois_data = rois_data.loc[rois_data[\"expected roi\"]>0,:]\n",
    "                                    plt.axhline(0, color=\"green\")\n",
    "                                    plt.axhline(-50, color=\"yellow\")\n",
    "                                    plt.axhline(-100, color=\"red\")\n",
    "                                    sns.boxplot(data=positive_rois_data, x=\"rate\", y=\"true roi\") # , showfliers = False\n",
    "                                    plt.title(f\"best rate (lowest risk)={best_rate} with worst roi of: {round(best_rate_min_roi,2)}\")\n",
    "                                    plt.savefig(f\"{portfolio_creation_training_path}/{model_name}.jpg\")\n",
    "\n",
    "\n",
    "                                    # Portfolio creation - interactive plot for testing data\n",
    "                                    features_dates = [i[0] for i in model_data.iloc[:,:-1].index]\n",
    "                                    features_stock = [i[1] for i in model_data.iloc[:,:-1].index]\n",
    "                                    features_predictions = model.predict(model_data.iloc[:,:-1], batch_size=BATCH_SIZE)\n",
    "                                    portfolio_data = np.array([features_dates,\n",
    "                                                               features_stock,\n",
    "                                                               model_data.iloc[:,:-1].iloc[:,-1],\n",
    "                                                               model_data.iloc[:,-1].values,\n",
    "                                                               features_predictions[:,0]]\n",
    "                                                              )\n",
    "                                    portfolio_data = pd.DataFrame(portfolio_data.T,\n",
    "                                                                  columns = [\"date\", \"stock\", \"current price\", \"true 1 year\", \"predicted 1 year\"])\n",
    "\n",
    "                                    portfolio_data = portfolio_data.sort_values(by=[\"date\", \"predicted 1 year\"], ascending=[True, False])\n",
    "                                    portfolio_data = portfolio_data.set_index(\"date\")\n",
    "                                    port_index = portfolio_data.index.unique()\n",
    "\n",
    "                                    port_index_tqdm = tqdm(enumerate(port_index), desc=\"Getting portfolio rois\", total=len(port_index))\n",
    "                                    rois = np.zeros((len(port_index),2))\n",
    "                                    extra_data = []\n",
    "                                    for i, date in port_index_tqdm:\n",
    "                                        if date in test_dates:\n",
    "                                            dataset_string = \"test\"\n",
    "                                        else:\n",
    "                                            dataset_string = \"train\"\n",
    "\n",
    "                                        # filter out relevant rows\n",
    "                                        current_df = portfolio_data.loc[date,:]\n",
    "\n",
    "                                        # Get array containing expected roi for each stock\n",
    "                                        expected_roi_per_stock = ((current_df[\"predicted 1 year\"].values - current_df[\"current price\"].values)/current_df[\"current price\"].values)*100\n",
    "                                        current_df[\"expected roi\"] = expected_roi_per_stock\n",
    "\n",
    "                                        current_df = current_df.sort_values([\"expected roi\"], ascending=False)\n",
    "                                        current_prices = current_df[\"current price\"]\n",
    "                                        current_stocks = current_df[\"stock\"]\n",
    "                                        expected_roi_per_stock = current_df[\"expected roi\"]\n",
    "\n",
    "                                        weights, num_stocks = portfolio_weights(best_rate, expected_roi_per_stock)\n",
    "\n",
    "                                        stocks_to_show = 10\n",
    "                                        weights_string = \" \".join(list([str(round(num, 2)) for num in list(weights)[:stocks_to_show]]))\n",
    "                                        stocks_string = \" \".join(list(current_stocks)[:stocks_to_show])\n",
    "                                        current_prices_string = \" \".join(list([str(round(num, 2)) for num in list(current_prices)[:stocks_to_show]]))\n",
    "                                        predicted_prices_string = \" \".join(list([str(round(num, 2)) for num in list(current_df[\"predicted 1 year\"])])[:stocks_to_show])\n",
    "                                        expected_roi_string = \" \".join(list([str(round(num, 2)) for num in list(expected_roi_per_stock)[:stocks_to_show]]))\n",
    "                                        true_price_string = \" \".join(list([str(round(num, 2)) for num in list(current_df[\"true 1 year\"])[:stocks_to_show]]))\n",
    "\n",
    "                                        current_portfolio_wealth = sum([weights[w] * decimal.Decimal(current_df[\"current price\"][w]) for w in range(len(weights))])\n",
    "                                        current_portfolio_wealth = float(current_portfolio_wealth)\n",
    "\n",
    "\n",
    "                                        future_true_wealth = sum([weights[w] * decimal.Decimal(current_df.loc[:, \"true 1 year\"][w]) for w in range(len(weights))])\n",
    "                                        future_true_wealth = float(future_true_wealth)\n",
    "\n",
    "\n",
    "                                        future_predicted_wealth = sum([weights[w] * decimal.Decimal(current_df.loc[:, \"predicted 1 year\"][w]) for w in range(len(weights))])\n",
    "                                        future_predicted_wealth = float(future_predicted_wealth)\n",
    "\n",
    "                                        if not current_portfolio_wealth == 0:\n",
    "                                            true_roi = ((future_true_wealth-current_portfolio_wealth)/current_portfolio_wealth)*100\n",
    "                                            expected_roi = ((future_predicted_wealth-current_portfolio_wealth)/current_portfolio_wealth)*100\n",
    "                                        else:\n",
    "                                            # The nans represent days when no stocks were expected to increase in price after 1 year\n",
    "                                            true_roi = np.nan\n",
    "                                            expected_roi = np.nan\n",
    "\n",
    "                                        rois[i, :] = np.array([true_roi, expected_roi])\n",
    "                                        # Append data\n",
    "                                        extra_data.append([stocks_string, current_prices_string, true_price_string, predicted_prices_string, expected_roi_string, num_stocks, weights_string, dataset_string])\n",
    "\n",
    "                                    rois_data = pd.DataFrame(rois, columns = [\"true roi\", \"expected roi\"], index=port_index)\n",
    "                                    extra_data = pd.DataFrame(extra_data,\n",
    "                                                              columns = [\"stocks\", \"current stock prices\", \"1Y true prices\", \"1Y predicted prices\", \"stocks expected roi\", \"num unique stocks\", \"weights\", \"dataset\"],\n",
    "                                                              index=port_index)\n",
    "                                    extra_data[\"date\"] = [str(date.date()) for date in extra_data.index]\n",
    "\n",
    "                                    rois_data = pd.concat([rois_data, extra_data], axis=\"columns\", ignore_index=False)\n",
    "                                    # Drop nan rows as those represent times when all stocks were expected to go down\n",
    "                                    rois_data = rois_data.dropna()\n",
    "\n",
    "                                    print(\"Creating interactive scatterplot\")\n",
    "                                    r_squared_train = r2_score(rois_data.loc[rois_data.dataset == \"train\", \"true roi\"], rois_data.loc[rois_data.dataset == \"train\", \"expected roi\"])\n",
    "                                    r_squared_test = r2_score(rois_data.loc[rois_data.dataset == \"test\", \"true roi\"], rois_data.loc[rois_data.dataset == \"test\", \"expected roi\"])\n",
    "                                    fig = px.scatter(rois_data,\n",
    "                                                     x=\"true roi\", y=\"expected roi\",\n",
    "                                                     title = f\"Train r2 = {round(r_squared_train,3)}, Test r2 = {round(r_squared_test,3)}\",\n",
    "                                                     hover_data=extra_data.columns,\n",
    "                                                     marginal_x=\"box\", marginal_y=\"box\",\n",
    "                                                     color = \"dataset\"\n",
    "                                                     )\n",
    "                                    # Add a line trace with y=x to the figure\n",
    "                                    fig.add_trace(go.Scatter(x=[rois_data[\"true roi\"].min(), rois_data[\"true roi\"].max()],\n",
    "                                                             y=[rois_data[\"true roi\"].min(), rois_data[\"true roi\"].max()],\n",
    "                                                             mode='lines',\n",
    "                                                             name='y=x'))\n",
    "\n",
    "                                    print(\"Saving interactive scatterplot\")\n",
    "                                    fig.write_html(f\"{portfolio_creation_testing_path}/{model_name}.html\")\n",
    "                                    print(\"Saved interactive scatterplot\")\n",
    "\n",
    "\n",
    "                                    # Add result to `results` dataframe and to \"model results.csv\"\n",
    "                                    total_neurons = layer * neuron\n",
    "                                    result_row = [features_string, NUMBER_OF_LINES, NUMBER_OF_YEARS_TO_CONSIDER, layer, neuron, total_neurons, BATCH_SIZE, technique, R2_score]\n",
    "                                    results = results.append({\"features\":features_string,\n",
    "                                                              \"lines\":NUMBER_OF_LINES,\n",
    "                                                              \"years\":NUMBER_OF_YEARS_TO_CONSIDER,\n",
    "                                                              \"layers\":layer,\n",
    "                                                              \"neurons per layer\":neuron,\n",
    "                                                              \"total neurons\":total_neurons,\n",
    "                                                              \"batch size\":BATCH_SIZE,\n",
    "                                                              \"technique\":technique,\n",
    "                                                              \"val_r_square\":R2_score}, ignore_index=True)\n",
    "\n",
    "\n",
    "                                    with open(f\"{root_folder}/{overview_folder}/model results.csv\", \"a\") as f:\n",
    "                                        writer = csv.writer(f)\n",
    "                                        writer.writerow(result_row)\n",
    "                                else:\n",
    "                                    print(f\"\\n\"\n",
    "                                          f\"ALREADY COMPLETED model:\\n\"\n",
    "                                          f\"Data: {data_name}\\n\"\n",
    "                                          f\"Layers: {layer}\\n\"\n",
    "                                          f\"Neurons: {neuron}\\n\"\n",
    "                                          f\"Batch size = {BATCH_SIZE}\\n\"\n",
    "                                          f\"Technique = {technique}\\n\")\n",
    "                                pbar.update(1)\n",
    "\n",
    "            # Now create the interactive plots\n",
    "            for col in results.columns[:-1]:\n",
    "                fig = px.scatter(results, x=col, y=\"val_r_square\",\n",
    "                                 color = \"features\",\n",
    "                                 size = list(results[\"total neurons\"]),\n",
    "                                 hover_data=results.columns[:-1],\n",
    "                                 title = f\"R2 score against {col}\"\n",
    "                                 )\n",
    "                fig.update_yaxes(range = [0, 1.05])\n",
    "                fig.write_html(f\"{root_folder}/{overview_folder}/R2 score against {col}.html\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Condition not met: NUMBER_OF_LINES <= NUMBER_OF_YEARS_TO_CONSIDER*365\")\n",
    "    else:\n",
    "        pbar.update(len(feature_combinations)*len(layers)*len(neurons)*len(batch_sizes)*len(techniques))\n",
    "\n",
    "p.close()\n",
    "p.join()\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
